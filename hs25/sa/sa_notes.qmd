---
title: "Classical and Bayesian Statistics - Notes"
toc: true
---

# Introduction

Statistic is the discipline that concerns collection, organisation, analysis,
interpretation, and presentation of data. Applied statistics applies to real everyday problems.

> Note: There is no cooking recipes how to solve problems.

# Classical Statistics

Classical statistics is a set of tools for decision making using hypothesis.

# Bayesien Statistic
Bayesian statistics is a statistical theory that interprets probability as a degree of belief in an event, which can be updated as new evidence is obtained.

> Note: Bayesian statistics is not the same as the Bayesian theorem.

# Models

Models are used to simplify things and are essential for statistics. However, models are only useful in a certain context. Models have their limitations. They are statements about how nature operates that deliberately omit many details, thus achieving insight that would otherwise be obscured.

# Simulations

Simulations are used to approximate quantities for which an exact solution would be very difficult, if not impossible, to determine. They rely heavily on computer power.

# Data

## One-Dimensional

List are the simplest kind od datasets. Lists are heterogeneous data structures.

## Two-Dimentinal

Tabels are the most common form of datasets

# Exploratory Datat Analysis (EDA)

The aim of EDA ist to summarize data by numerical parameters and graphical representation of data. Data should if possible always be graphically displayed and compared with corresponding key figures.

> Note:  Note: whenever a dataset is reduced by key figures or graphics, information gets lost.

# Key Figures

## Location parameters

- Arithemtic mean (average)
- Median
- Quantil

## Spread parameters

- Empirical variance
- Stadnard deviation
- Interquantile range

## Artithmetic Mean

Mean tells a lot about a dataset: “Center” of data. But average does not tell whole story about (quantitative) datasets. Datasets can have a different spread around mean.

$$
\bar{x} = \dfrac{x_1 + x_2 + \dots{} + x_n}{n} = \dfrac{1}{n}\sum^n_{i=1}x_i
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
mean(x)
```

> Note: The arithmetic mean is not robust to outliers.

## Empirical Variancehave

The value of empirical variance has no physical interpretation.

$$
Var(x) = \dfrac{(x_1-\bar{x})² + (x_2-\bar{x})² + \dots + (x_n-\bar{x})² }{n-1} = \dfrac{1}{n-1}\sum^n_{i=1}(x_i - \bar{x}²)
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
var(x)
```

## Standard Deviation

Standard deviation is root of variance. Standard deviation has same unit as data itself.

$$
s_x = \sqrt{Var(x)} = \sqrt{\dfrac{1}{n-1}\sum^n_{i=1}(x_i - \bar{x}²)}
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
sd(x)
```

## Median
Also called central value or average value. Median is much less influenced by extreme observations than mean.

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
median(x)
```

> Note: Consider Mean and Median simultaneously instead of choosing one.

## Quantiles
Quantiles are values that divide a dataset into equal parts, allowing for the analysis of the distribution of data. Common types of quantiles include quartiles which split data into four parts.

> Note: Most of the time there is no exact 25 % of observations.

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
quantile(x) # Default quartil
quantile(x, p = 0.7) # Individual value
```

# Interquantile Range

Measure for spread of data.

$$
\text{upper quartile} − \text{lower quartile}
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
IQR(x)
```

# Graphical Representation of Data

Plotting data is very important aspect of statistical data analysis. Often points to patterns that are not recognisable from key figures.

> Note: Choosing “wrong” graphical representation is not useful.

## Boxplot

![Boxplot: Schematically](img/boxplot.png)

- Box: Height is bounded by lower and upper quartile. Height of box is interquartile rang.
- Horizontal line in box: Median
- Whisker: $1.5 \times \text{IQR}$. Defined by inventor John Tukey.
- Points: Outliers

> Note: Upper and lower whisker do not have to have the length of $1.5 \times \text{IQR}$.

```{r}
# Single boxplot
set.seed(42) # Set seed for reproducibility
x <- rnorm(10) # Generate 10 random samples

boxplot(x) # Plot data
```

```{r}
# Compare two sample groups
set.seed(42)
x <- rnorm(100)
y <- rnorm(100)

boxplot(x, y)
```

## Histogram

Graphical overview of occurring values. Draw a bar for each class. Height proportional to number of observations in that class.

```{r}
# Simple histogram
set.seed(42)
x <- rnorm(1000)

hist(x)
```

Selection of number of classes relevant for interpretation of histogram.

```{r}
# Big breaks
set.seed(42)
x <- rnorm(1000)

hist(x, breaks=5)
```

```{r}
# Small breaks
set.seed(42)
x <- rnorm(1000)

hist(x, breaks=50)
```

> Note: Since we used `set.seed(42)`, both plot shows the same data.

### Skweness of Hostograms

Histograms can have a skweness depending on the data.

```{r}
# Right skewed data
set.seed(42)
x <- rexp(1000, rate = 1) # Using expontial distribution

hist(x, breaks=50)
```

```{r}
# Left left data
set.seed(42)
x <- rexp(1000, rate = 1)
x <- -x # Trun data positivity

hist(x, breaks=50)
```

> Note: The terms “right” and “left” always refers to direction where it has less
data (tail of distribution).

## Normalised Histograms

Select bar height such that bar area corresponds to proportion of respective observations in total number of observations.

> Note: Density values are not percentages.

```{r}
# Normalized histogram
set.seed(42)
x <- rnorm(1000)

hist(x, freq=FALSE) # Disable frequency
```


## Boxplot X Histogram

![Boxplot X Histogram](img/boxplot_x_histogram.png)

# Probability

- A random element: $x = 2$
- Set of elements: $X = {1, 2, 3, 4}$
- x is a element of the set X: $x \in X$
- Subset: If $B = {2, 3}$, than $B \subset A$
- Empty set: $\emptyset$. The empty subset is a supset of every set.

## Probability Model

Probability models use set theory as language. For random experiments the outcome is not predictable. Probability model consists of events that are possible in such an experiment and probabilities for different results occurring. Probability models have following components:

- Sample space $\Omega$: Contains all possible elementary events $\omega$.
- Events $A, B, C$: Subsets of sample space.
- Probabilities $P$ associated with events $A, B, C$
- Event: More general and more important than elementary events, but consist of these.

Probability formula:

$$
P(E)=\dfrac{\lvert E \rvert}{\lvert \Omega \rvert} \text{, where } E \subseteq \Omega
$$

## Set Theory

- Union: $A \cup B$
- Intersection: $A \cap B$
- Complement: $\bar{A}$
- Difference: $A \setminus B$

![Set theory](img/set_theory.png)

## Axioms of Probability

Each event $A$ probability $P(A)$ is assigned, with properties:

- $P(A) \geq 0$
- $P(\Omega) = 1$
- $P(A \cup B) = P(A) + P(B) \text{, if } A \cap B = \emptyset$

> Note: Kolmogorov Axioms of Probability

## Laws for Calculating Probabilities

If $A, B$ and $A_1, \dots, A_n$ events, then:

- $P(\bar{A}) = 1 - P(A) \text{, for all } A$
- $P(A \cup B) = P(A) + P(B) - P(A \cap B) \text{, for all } A \text{ and } B$
- $P(A_1 \cup \dots A_n) \leq P(A_1) + \dots + P(A_n) \text{, for all } A_1, \dots, A_n$
- $P(B) \leq P(A) \text{, for all } A \text{ and } B \text{ with } B \subseteq A$
- $P(A \setminus B) = P(A) - P(B) \text{, for all } A \text{ and } B \text{ with } B \subseteq A$

## Discrete Probability Models

Sample space finite or infinite and discrete. Infinite, but still discrete set.

### Probabilities for Discrete Models

Probability of event $A = {\omega_1, \omega_2, \dots, \omega_n}$ is determined by sum of probabilities P(ω) of corresponding ele-
mentary events

$$
P(A) = \sum_{\omega_i \in A} P(\omega_i)
$$


## Laplace Model

Probabilities of all elementary elements add up to 1.

$$
P(E) = \dfrac{f}{p} = \sum_{k:\omega_i \in E} P(\omega_k)
$$

Divides number of “favorable” elementary events by number of “possible” elementary events.

## Stochastic Independence

If events $A$ and $B$ are stochastically independent, then:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

> Note: Formula applies only if events A and B are stochastically independent

Outcome of event A has no influence on outcome of event B and vice versa.