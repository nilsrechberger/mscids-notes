---
title: "Classical and Bayesian Statistics - Notes"
toc: true
---

# Introduction

Statistic is the discipline that concerns collection, organisation, analysis,
interpretation, and presentation of data. Applied statistics applies to real everyday problems.

> Note: There is no cooking recipes how to solve problems.

# Classical Statistics

Classical statistics is a set of tools for decision making using hypothesis.

# Bayesien Statistic
Bayesian statistics is a statistical theory that interprets probability as a degree of belief in an event, which can be updated as new evidence is obtained.

> Note: Bayesian statistics is not the same as the Bayesian theorem.

# Models

Models are used to simplify things and are essential for statistics. However, models are only useful in a certain context. Models have their limitations. They are statements about how nature operates that deliberately omit many details, thus achieving insight that would otherwise be obscured.

# Simulations

Simulations are used to approximate quantities for which an exact solution would be very difficult, if not impossible, to determine. They rely heavily on computer power.

# Data

## One-Dimensional

List are the simplest kind od datasets. Lists are heterogeneous data structures.

## Two-Dimentinal

Tabels are the most common form of datasets

# Exploratory Datat Analysis (EDA)

The aim of EDA ist to summarize data by numerical parameters and graphical representation of data. Data should if possible always be graphically displayed and compared with corresponding key figures.

> Note:  Note: whenever a dataset is reduced by key figures or graphics, information gets lost.

# Key Figures

## Location parameters

- Arithemtic mean (average)
- Median
- Quantil

## Spread parameters

- Empirical variance
- Stadnard deviation
- Interquantile range

## Artithmetic Mean

Mean tells a lot about a dataset: “Center” of data. But average does not tell whole story about (quantitative) datasets. Datasets can have a different spread around mean.

$$
\bar{x} = \dfrac{x_1 + x_2 + \dots{} + x_n}{n} = \dfrac{1}{n}\sum^n_{i=1}x_i
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
mean(x)
```

> Note: The arithmetic mean is not robust to outliers.

## Empirical Variancehave

The value of empirical variance has no physical interpretation.

$$
Var(x) = \dfrac{(x_1-\bar{x})² + (x_2-\bar{x})² + \dots + (x_n-\bar{x})² }{n-1} = \dfrac{1}{n-1}\sum^n_{i=1}(x_i - \bar{x}²)
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
var(x)
```

## Standard Deviation

Standard deviation is root of variance. Standard deviation has same unit as data itself.

$$
s_x = \sqrt{Var(x)} = \sqrt{\dfrac{1}{n-1}\sum^n_{i=1}(x_i - \bar{x}²)}
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
sd(x)
```

## Median
Also called central value or average value. Median is much less influenced by extreme observations than mean.

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
median(x)
```

> Note: Consider Mean and Median simultaneously instead of choosing one.

## Quantiles
Quantiles are values that divide a dataset into equal parts, allowing for the analysis of the distribution of data. Common types of quantiles include quartiles which split data into four parts.

> Note: Most of the time there is no exact 25 % of observations.

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
quantile(x) # Default quartil
quantile(x, p = 0.7) # Individual value
```

# Interquantile Range

Measure for spread of data.

$$
\text{upper quartile} − \text{lower quartile}
$$

```{r}
x <- c(4.3, 5.2, 2.7, 3.1)
IQR(x)
```

# Graphical Representation of Data

Plotting data is a very important aspect of statistical data analysis. It often points to patterns that are not recognizable from key figures.

> Note: Choosing the “wrong” graphical representation is not useful.

## Boxplot

![Boxplot: Schematically](img/boxplot.png)

- Box: Height is bounded by the lower and upper quartiles. The height of the box is the interquartile range.
- Horizontal line in box: Median
- Whisker: $1.5 \times \text{IQR}$, defined by inventor John Tukey.
- Points: Outliers

> Note: The upper and lower whiskers do not have to be $1.5 \times \text{IQR}$ in length.

```{r}
# Single boxplot
set.seed(42) # Set seed for reproducibility
x <- rnorm(10) # Generate 10 random samples

boxplot(x) # Plot data
```

```{r}
# Compare two sample groups
set.seed(42)
x <- rnorm(100)
y <- rnorm(100)

boxplot(x, y)
```

## Histogram

Graphical overview of occurring values. Draw a bar for each class, with the height proportional to the number of observations in that class.

```{r}
# Simple histogram
set.seed(42)
x <- rnorm(1000)

hist(x)
```

The selection of the number of classes is relevant for the interpretation of a histogram.

```{r}
# Big breaks
set.seed(42)
x <- rnorm(1000)

hist(x, breaks=5)
```

```{r}
# Small breaks
set.seed(42)
x <- rnorm(1000)

hist(x, breaks=50)
```

> Note: Since we used `set.seed(42)`, both plots show the same data.

### Skweness of Hostograms

Histograms can have a skewness depending on the data.

```{r}
# Right skewed data
set.seed(42)
x <- rexp(1000, rate = 1) # Using expontial distribution

hist(x, breaks=50)
```

```{r}
# Left left data
set.seed(42)
x <- rexp(1000, rate = 1)
x <- -x # Trun data positivity

hist(x, breaks=50)
```

> Note: The terms “right” and “left” always refer to the direction where there is less data (the tail of the distribution).

## Normalised Histograms

Select the bar height such that the bar area corresponds to the proportion of respective observations in the total number of observations.

> Note: Density values are not percentages.

```{r}
# Normalized histogram
set.seed(42)
x <- rnorm(1000)

hist(x, freq=FALSE) # Disable frequency
```

## Boxplot X Histogram

![Boxplot X Histogram](img/boxplot_x_histogram.png)

# Probability

- A random element: $x = 2$
- Set of elements: $X = {1, 2, 3, 4}$
- $x$ is an element of the set $X$: $x \in X$
- Subset: If $B = {2, 3}$, then $B \subset A$
- Empty set: $\emptyset$. The empty set is a subset of every set.

## Probability Model

Probability models use set theory as a language. For random experiments, the outcome is not predictable. A probability model consists of events that are possible in such an experiment and probabilities for different results occurring. Probability models have the following components:

- Sample space $\Omega$: Contains all possible elementary events $\omega$.
- Events $A, B, C$: Subsets of the sample space.
- Probabilities $p$ associated with events $A, B, C$.
- Event: More general and more important than elementary events, but consists of them.

Probability formula:

$$
p(E)=\dfrac{\lvert E \rvert}{\lvert \Omega \rvert} \text{, where } E \subseteq \Omega
$$

## Set Theory

- Union: $A \cup B$
- Intersection: $A \cap B$
- Complement: $\bar{A}$
- Difference: $A \setminus B$

![Set theory](img/set_theory.png)

## Axioms of Probability

Each event $A$ probability $P(A)$ is assigned, with properties:

- $p(A) \geq 0$
- $p(\Omega) = 1$
- $p(A \cup B) = p(A) + p(B) \text{, if } A \cap B = \emptyset$

> Note: Kolmogorov Axioms of Probability

## Laws for Calculating Probabilities

If $A, B$ and $A_1, \dots, A_n$ events, then:

- $p(\bar{A}) = 1 - p(A) \text{, for all } A$
- $p(A \cup B) = p(A) + p(B) - p(A \cap B) \text{, for all } A \text{ and } B$
- $p(A_1 \cup \dots A_n) \leq p(A_1) + \dots + p(A_n) \text{, for all } A_1, \dots, A_n$
- $p(B) \leq p(A) \text{, for all } A \text{ and } B \text{ with } B \subseteq A$
- $p(A \setminus B) = p(A) - p(B) \text{, for all } A \text{ and } B \text{ with } B \subseteq A$

## Discrete Probability Models

A sample space can be finite or infinite and discrete. It can also be infinite but still discrete.

### Probabilities for Discrete Models

The probability of event $A = {\omega_1, \omega_2, \dots, \omega_n}$ is determined by the sum of the probabilities $p(\omega)$ of the corresponding elementary events.

$$
p(A) = \sum_{\omega_i \in A} p(\omega_i)
$$


## Laplace Model

Probabilities of all elementary elements add up to 1.

$$
p(E) = \dfrac{f}{p} = \sum_{k:\omega_i \in E} p(\omega_k)
$$

Divides number of “favorable” elementary events by number of “possible” elementary events.

## Stochastic Independence

If events $A$ and $B$ are stochastically independent, then:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

> Note: Formula applies only if events A and B are stochastically independent.

Outcome of event A has no influence on outcome of event B and vice versa.

# Random Variable & Probability Distribution

## Random Variable

A Random variable X is a function:

$$
\Omega \to W_X \subset \mathbb{R}
$$
$$
\omega \to X(\omega)
$$

Random variable denoted by capital letters X (or Y, Z). Corresponding lowercase letters x (or y, z) represents specific value that random variable can take. Once $\omega$ is chosen: $X(\omega)$ is fixed, not random.

> Note: $x$ also called a realisation of random variable $X$.

## Probability Distribution of Random Variable

Values of random variable X (possible realisations of X) occur with certain probabilities that $X$ takes value $x$:

$$
P(X = x) = P(\{\omega \mid X(\omega) = x\}) = \sum_{\omega; X(\omega)=x} P(\omega)
$$


## Probability Distribution

Associated probability is determined for all realisations of random variable. List of $P(X = x)$ for all possible values $x_1, x_2, \dots, x_n$ is called discrete probability distribution of discrete random variable $X$.All values of probability distribution has to sum up to 1.

$$
\sum_{\text{For all }x} P(X = x) = 1
$$

> Note: For finite sample space the probability distribution is a table.

## Key Figures of Distribution

### Expected Value $E(X)$

Central location of distribution. Weighted mean of all possible values, weighted by their probability of occurring.

$$
\mu = \sum_{\text{all possible } x} x \times P(X = x)
$$

> Note: $E(X)$ is a theoretical value, which results from a model, i.e. distribution.

### Variance

Variance is square of spread of value of random variable from expected value weighted with respective weight.

$$
\sigma = \sum_{\text{all possible } x} (x - E(X))^2 \times P(X = x)
$$

### Standard deviation $\sigma$

Spread of distribution about $E(X)$. Standard deviation has same unit as X.

$$
\sigma(X) = \sqrt{Var(X)}
$$

> Note: $\sigma(x)$ is a theoretical value, which results from a model, i.e. distribution.


```{r}
x <- 1 : 6
p <- 1 / 6
E_X <- sum(x * p)
var_X <- sum((x - E_X)^2 * p)
sd_X <- sqrt(var_X)

sd_X
```

> Note: Means: Deviation on “average” 1.7 from 3.5.

# Week 04: Continuous Distributions & Normal Distribution

## Continuous Probability Distribution

For continuous probability distributions, probabilities correspond to areas under density function. Range $W_X$ of a random variable: Set of all values $X$ can take. Random variable $X$ is continuous, if its range $W_X$ is continuous. For continuous random variable $X$ for all $x \in W_X$:

$$
P(X = x) = 0
$$

> Note: Probability distribution of $X$ can not be described by $P(X = x)$.

## Probability Density

Probability density function f(x) has the properties:

- Function is not negative: $f(x) \ge 0$
- Probability corresponds to area between $a$ and $b$ under $f(x)$: $P(a \lt X \le b )$
- Total area under curve is $1$

> Note: Values of f(x) are not probabilities, only areas are.

## Quantiles of Continuous Probability Distribution

For continuous distributions, the $\alpha$ quantile $q_\alpha$ is value where area (probability) under density function from $-\infty$ to $q_\alpha$ is just $\alpha$.

![Quantiles of Continuous Probability Distribution](img/contProbDistrAlpha.png)

## Normal Distribution

The normal distribution $X \sim \mathcal{N}(\mu, \sigma^2)$ have the desity function:

$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{1}{2} \left(\frac{x - \mu}{\sigma}\right)^2 \right\}
$$

With the expected value $E(X) = \mu$ and variance $Var(X) = \sigma^2$. The parameter $\mu$ shifts the curve horizontally from origin while $\sigma$ defines the shape of the curve.

### Properties

- Approx. $\frac{2}{3}$ of are between $\mu \pm \sigma$
- Approx. 95% of area between $\mu \pm 2\sigma$

### Calculate Normal Distributed Probability

Calculation of IQ $P(X \le 130)$:

```{r}
# mean and sd are predefined in this task
pnorm(q = 130, mean = 100, sd = 15)
```

since total area under curve is 1, $P(X \gt 130)$:

```{r}
1 - pnorm(q = 130, mean = 100, sd = 15)
```

Determine quantiles:

```{r}
qnorm(p = 0.025, mean = 100, sd = 15)
qnorm(p = 0.975, mean = 100, sd = 15)
```

## i.i.d Assumption

The i.i.d. assumption means a set of random variables are Independent (the value of one doesn't affect the others) and Identically Distributed (all come from the exact same probability distribution).

### Key Figures

#### $S_n$

- $E(S_n) = n\mu$
- $\text{Var}(S_n) = n\text{Var}(X_i)$
- $\sigma(S_n) = \sqrt{n}\sigma_X$

#### $\bar{X}_n$

- $E(\bar{X}_n) = \mu$
- $\text{Var}(\bar{X}_n) = \frac{\sigma^2_X}{n}$
- $\sigma(\bar{X}_n) = \frac{\sigma_X}{\sqrt{n}}$

> Note: Standard deviation of $X_n$ is called standard error of arithmetic mean.


## Central Limit Theorem

The Central Limit Theorem (CLT) states that, given a sufficiently large sample size ($n$), the distribution of the sample mean ($\bar{X}_n$) will be approximately normally distributed, regardless of the original population's distribution. This is true as long as the population has a finite mean ($\mu$) and variance ($\sigma^2$). In short: For large samples, the sample mean is normally distributed.

# Week 05: Hypothesis Tests

Hypothesis testing are a important statistical tool to decide whether observations “fits” a certain parameter. Ro introduce a standardised, reproducible procedure to decide
whether mean of observations does (or not) match a certain true mean $\mu$.

> Note: We can only show that this quantity does not fit to the observations with high probability.

## Estimation

Point estimates for expected value.

$$
\hat{\mu} = \dfrac{1}{n}\sum^n_{i=1}X_i
$$

Point estimates for variance.

$$
\hat{\sigma}^2_X = \dfrac{1}{n-1}\sum^n_{i=1}(X_i - \bar{X_n})^2
$$

> Note: Hat $\hat{x}$ denotes estimate of a quantity.

## Procedure Hypothesis Test

Using a observation, check whether, under assumption $\mu = x$, mean of observations is probable or not.

- $\mu$: True (unknown) mean of data
- $\mu_0$: Assumed true mean of data

Null Hypothesis

$$
H_0: \mu = \mu_0 = x
$$

Alternative Hypothesis

$$
H_A: \mu \ne \mu_0 = x
$$

Test with this distribution whether assumption μ = x is justified. Distribution of test statistic $T$ under the null hypothesis $H_0$

$$
T: \bar{X}_{10} \sim \mathcal{N}(x, \dfrac{1^2}{x})
$$

```{r}
# Assume real mean of 500
pnorm(q = 499.22, mean = 500, sd = 1/sqrt(10))
```

It has proven practical to set this limit of what is too small and what is not at 2.5 %. So assume that given mean of $μ_0 = 500$ is not plausible. We reject null hypothesis.

## Significance Level \alpha

Significance level $\alpha$, indicates how high a risk one is willing to take of making a wrong decision. For most tests $\alpha$ value of $0.05$ or $0.01$. Boundary rejection range $0.025$- and $0.975$-quantiles.

```{r}
qnorm(p = c(0.025, 0.975), mean = 500, sd = 1/sqrt(10))
```

If observed mean lies in red area of Figure, null hypothesis is rejected, also called rejection range.

## p-Value

p-value is probability of observing an event under null hypothesis that is at least as extreme (in direction of alternative) as currently observed event. The smaller p-value, the more result argues against null hypothesis. Values smaller than a predetermined limit, such as 5 %, 1 % or 0.1 % are reason to reject the null hypothesis.
