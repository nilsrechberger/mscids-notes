---
title: "Classical and Bayesian Statistics - Exercises"
toc: true
format:
  html: default
  pdf:
    include-in-header:
      - text: |
          \newcommand{\lt}{<}
          \newcommand{\gt}{>}
---

# Problems 1

## a

It seems that, in total, the Allied and Axis countries had about the same number of civilian deaths. However, if we compare the numbers per party member, we see that the Axis countries had a higher average amount of deaths. On the other hand, the data from the Allies is inconsistent (see Denmark).

## b

The percentages in the chart do not sum up to 100%.

## c

### I

We can assume that Linda is a bank teller and is active in the feminist movement. Her experiences from the past could have influenced her behavior and thinking on certain topics. She may have a sense for justice and equality, regardless of the topic.

### II

We can assume that Steve is a librarian because of his helpful personality and his need for order and structure, much like what's found in a library. His passion for details may also be connected to a desire for knowledge.

### III

A Ball costs $0.05.

### IV

There are more death by heart diseases than accidents.

## d

Since the engine is the only thing that keeps a plane in the air, it makes sense for it to be more armoured than the rest of the plane. Even if other parts have more bullet holes on average (e.g. the fuselage), the plane could still fly.

# Problem 1.2

```{r}
# Define vectors
winner <- c(193, 183, 191, 185, 185, 182, 182, 188, 188, 188, 185, 185, 177,
182, 182, 193, 183, 179, 179, 175)

opponent <- c(163, 191, 165, 187, 175, 193, 185, 187, 188, 173, 180, 177, 183,
185, 180, 180, 182, 178, 178, 173)
```

### a

```{r}
# Determine length
cat("Length of vector winners =", length(winner), "\n") # Add line break
cat("Length of vector opponent =", length(opponent))
```

### b

```{r}
cat("Entries 6 to 10 =",winner[6:10]) # Index starts at 1
```

### c

```{r}
cat("Some values from winner:", winner[c(3, 5, 10, 12)]) # Passing a vector for selection
```

### d

```{r}
cat("Current values:",winner[c(8, 9)], "\n")  # Check values
winner[c(8, 9)] <- 189  # Reassign 
cat("New values:", winner[c(8, 9)])  # Check values
```

### e

```{r}
mu_winner <- mean(winner)
mu_opponent <- mean(opponent)

cat("Mean higth of winner vs. opponent:", mu_winner, "vs.", mu_opponent)
```

### f

```{r}
mu_diff <- mu_winner - mu_opponent

cat("Differences between means =", mu_diff)
```

### g

```{r}
var_winner <- var(winner)
sd_winner <- sd(winner)

cat("Variance / Std. deviation of winner:", var_winner, "/", sd_winner)
```

### h

```{r}
my_variance <- function(data){
    mu <- mean(data)
    sum_of_squares <- sum((data - mu)^2)
    variance <- sum_of_squares / (length(data) - 1)
    
    return(variance)
}

my_stdDeviation <- function(variance){
    stdDevition <- sqrt(variance)

    return(stdDevition)
}

my_var_winner <- my_variance(winner)
my_sd_winner <- my_stdDeviation(my_var_winner)

cat("Variance of Winner =", my_var_winner, "\n") # Add line break
cat("Variance of Winner =", my_sd_winner)
```

## Problem 1.3

```{r}
grades <- c(4.2, 2.3, 5.6, 4.5, 4.8, 3.9, 5.9, 2.4, 5.9, 6, 4, 3.7, 5, 5.2, 4.5, 3.6, 5, 6, 2.8, 3.3, 5.5, 4.2, 4.9, 5.1)

grades <- sort(grades) # Sort values

original_mu <- mean(grades)
originla_meadian <- median(grades)

grades[9:11] <- 1 # Reassign values
new_mu <- mean(grades)
new_median <- median(grades)

cat("Original vs. New mean:", original_mu, "vs.", new_mu, "\n") # Add line break
cat("Median stays the same:", originla_meadian, "=", new_median)
```

# Problems 2

## Probmel 2.1

### a

```{r}
# Read data
data <- read.csv('/home/nils/dev/mscids-notes/hs25/sa/data/husband_wife.csv')
head(data)
```

### b

```{r}
summary(data)
```

For each column, we see a brief summary with quantitative and qualitative information about the data.

### c

```{r}
age_diff <- data$age.husband - data$age.wife # Calc age difference

boxplot(age_diff)
```

### d

- The median of age_diff is about 2.5. On average, the age difference between husbands and wives is around 2.5 years.
- 50% of the differences lie between approximately 0 and 5 years.
- There are more upper than lower outliers, meaning that extreme cases where the husband is much older than the wife occur more frequently.
- In addition, the values of the upper outliers are larger than those of the lower ones.

## Problem 2.2

```{r}
head(InsectSprays) # Preview data from head
```

### a

```{r}
tapply(InsectSprays$count, InsectSprays$spray, mean)
```

### b

```{r}
boxplot(count ~ spray,
        data = InsectSprays)
```

## Problem 2.3

```{r}
data <- read.csv('/home/nils/dev/mscids-notes/hs25/sa/data/Diet.csv')
head(data)
```

```{r}
# Add column weight.los
data$weight.loss <- data$weight6weeks - data$pre.weight
data$weight.loss
```

```{r}
tapply(data$weight.loss, data$Diet, mean)
```

According to the data, diet 3 appears to have the greatest effect on weight loss over the 6-week therapy period. Diets 1 and 2 show more or less the same effect, although patients following diet 2 lost slightly less weight on average.

```{r}
boxplot(data$weight.loss ~ data$Diet)
```

- Even though diet 3 appears to have the greatest effect according to the median, it also has the largest interquartile range (IQR) among the three diets.
- Diet 2 shows the greatest overall spread across the entire boxplot
- Diet 1 is influenced by several lower outliers.

## Problem 2.4

### a

The probabilities of 'heads' and 'tails' do not add up to 1.

### b

The calculated probability is negative. That's not possible by definition.

### c

The union of the quantities `S` and `M` cannot be 0.7, because men cannot be pregnant.

## Problem 2.5

### a

Sample space of the experiment:

$$
\Omega = \{ (i, j) \mid i, j \in \{1,2,3,4,5,6\} \}
$$

### b

$$
p(\omega_n) = \dfrac{1}{36} = 0.02\bar{7}
$$

### c

Events, where the sum is 7:

$$
E_1 = {(1, 6), (2, 5), (3, 4)}
$$

> Note: Since there are two dices, we can multiply the number of favourable results by 2.

Now, we can calculate the probability:

$$
p(E_1) = \dfrac{6}{36} = 0.1\bar{6}
$$

### d

$$
E_2 = {(1, 1), (1, 2), (2, 1)}
$$

$$
p(E_2) = \dfrac{3}{36} = 0.08\bar{3}
$$

### e

$$
E_3 = \{ (i, j) \mid i, j \in \{1,3,5\} \}
$$

$$
p(E_3) = \dfrac{9}{36} = 0.25
$$

### f

```{r}
p_e2 <- 3/36
p_e3 <- 9/36
p_intersection <- 1/36

p_annual <- p_e2 + p_e3 - p_intersection

print(p_annual)
```

## Problem 2.6

```{r}
p_A <- 3/4
p_B <- 2/3
```

### a

```{r}
p_bothEvents <- p_A * p_B
print(p_bothEvents)
```

### b

```{r}
p_atLeastOne <- p_A + p_B - p_A * p_B
print(p_atLeastOne)
```

### c

```{r}
p_atMostOne <- 1 - p_A * p_B

cat(p_atMostOne)
```

### d

```{r}
p_noEvent <- 1 - (p_A + p_B - p_A * p_B)
print(p_noEvent)
```

### e

```{r}
p_exactlyOneEvent <- p_A + p_B - 2 * p_A * p_B
print(p_exactlyOneEvent)
```

## Problem 2.7

```{r}
p_earthquake <- 0.04
p_typhoon <- 0.08

p_annual <- p_earthquake + p_typhoon - p_earthquake * p_typhoon

print(p_annual)
```

# Problems 3

## Problem 3.1

$$
p_2 = 1 - 0.3 - 0.1 - 0.2 - 0.3 = 0.1
$$

## Problem 3.2

### a

The probabilities in the table sum to one, so it is a probability distribution.

$$
\sum P(X = k) = 1
$$

### b

$$
p(2 \le k \le 4) = 0.2 + 0.2 + 0.1 = 0.5
$$

### c

$$
p(k \gt 2) = 0.2 + 0.1 + 0.1 = 0.4
$$


### d

$$
p(k \le 4) = 1 - 0.1 = 0.9
$$


### e

$$
p(k \gt 1) = 1 - 0.4 = 0.6
$$


## Problem 3.3

### a

$$
p(k \le 13) = 0.992
$$

### b

$$
p(k \ge 10) = 1 - 0.939 = 0.061
$$

### c

$$
p(k = 15) = 1 - 0.999 = 0.001
$$

### d

$$
p(9 \le k \le 12) = 0.989 - 0.711 = 0.282
$$

## Problem 3.4

### a

$$
\Omega = \{\text{TTT}, \text{TTH}, \text{THT}, \text{HTT}, \text{THH}, \text{HTH}, \text{HHT}, \text{HHH}\}
$$
$$
P(X = 0) = \frac{1}{8}
$$
$$
P(X = 1) = \frac{3}{8}
$$
$$
P(X = 2) = \frac{3}{8}
$$
$$
P(X = 3) = \frac{1}{8}
$$

### b

$$
p(x = 2) = \frac{3}{8}
$$

### c

$$
p(X \ge 2) = \frac{3}{8} + \frac{1}{8} = \frac{1}{2}
$$

### d

$$
p(X \le 1) = \frac{1}{8} + \frac{3}{8} = \frac{1}{2}
$$

## Problem 3.5

```{r}
x_k <- c(-5, -4, 1, 3, 6)
p <- c(0.3, 0.1, 0.2, 0.3)
p_k <- 1 - sum(p) # Calc p_k
cat("Probability of -4 =", p_k, "\n")

p <- c(0.3, p_k, 0.1, 0.2, 0.3) # Reassign p

mu <- sum(x_k * p) # Calc expected value
mu
```

## Problem 3.6

### a

$$
p(x) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}
$$

### b

```{r}
x <- 2:12 # Sum of eyes
p <- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36 # Probability of summed eyes

mu = sum(x * p)
mu

var = sum((x - mu)**2 * p)
var

sd = sqrt(var)
sd
```

# Problems 4

## Problem 4.2

```{r}
# Define distr. paras
mu <- 4
sd <- 1.25
```

### a

```{r}
# Assume normal distr.
pnorm(q = 2.5, mean = mu, sd = sd)
```

### b

```{r}
1 - pnorm(q = 5.0, mean = mu, sd = sd)
```

### c

```{r}
pnorm(q = 4.5, mean = mu, sd = sd) - pnorm(q = 3.0, mean = mu, sd = sd)
```

### d

```{r}
qnorm(0.98, mean=mu, sd=sd)
```

## Problem 4.3

```{r}
# Define distr. paras
mu <- 2.2
sd <- 0.3

# Assume normal distr.
1 - pnorm(q=3.1, mean=mu, sd=sd/sqrt(100))
```

## Problem 4.4

```{r}
# Define distr. paras
mu <- 8.2
sd <- 6.0
```

### a

```{r}
# Assume normal distr.
pnorm(q=10.0, mean=mu, sd=sd/sqrt(36))
```

### b

```{r}
pnorm(q=10.0, mean=mu, sd=sd/sqrt(36)) - pnorm(q=5.0, mean=mu, sd=sd/sqrt(36))
```

### c

```{r}
1- pnorm(q=20.0, mean=mu, sd=sd/sqrt(36))
```

### d

It's small, but not impossible. We also assume a normal distribution. The real distribution probability may differ from the normal distribution. We also use a very small sample size of 36.

### e

Yes, the i.i.d. assumption holds here because each of the 36 passengers is an individual who is independent of the others.

## Problem 4.5

```{r}
# Define distr. paras
mu <- 77
sd <- 15

course_1 <- 25
course_2 <- 64
```

### a

```{r}
# Assume normal distr.
pnorm(q=82, mean=mu, sd=sd/sqrt(course_1)) - pnorm(q=72, mean=mu, sd=sd/sqrt(course_1))
```

### b

```{r}
pnorm(q=82, mean=mu, sd=sd/sqrt(course_2)) - pnorm(q=72, mean=mu, sd=sd/sqrt(course_2))
```

For a larger group, the probability is more likely to be at the mean compared to a smaller group (CLT).

# Problems 5

## Problem 5.1

- $H_0$: $\mu = \mu_0 = 70$
- $H_A$: $\mu \lt 70$

Rejection range:

```{r}
data <- c(71, 69, 67, 68, 73, 72, 71, 71, 68, 72, 69, 72)
sd <- 1.5

mu_hat <- mean(data)

qnorm(p = 0.05, mean = 70, sd = 1.5/sqrt(12))
```

Test:

```{r}
pnorm(q = mu_hat, mean = 70, sd = 1.5 / sqrt(12))
```

We do not reject the null hypothesis.

- p-value: 0.718

The mean of the sample does not statistical deviate from the producers claimed mean.

## Problem 5.2

### a

- $H_0$: $\mu = \mu_0 = 50$
- $H_A$: $\mu \lt 50$

```{r}
data <- c(46, 48, 52, 49, 46, 51, 52, 47, 49, 44, 48, 51, 49, 50, 53, 47)
sd <- 3.0

mu_hat <- mean(data)

pnorm(q = mu_hat, mean = 50, sd = 3.0/sqrt(16))
```

We do not reject the null hypothesis.

- p-value: 0.0668072

### b

```{r}
data <- c(46, 48, 52, 49, 46, 51, 52, 47, 49, 44, 48, 51, 49, 50, 53, 47)
sd <- 3.0

mu_hat <- mean(data)

pnorm(q = mu_hat, mean = 50, sd = 3.0/sqrt(100))
```

We reject the null hypothesis.

# Problems 6

## Problem 6.1

### a

- Paired samples: We use the same people for the before and after smoking measurements.
- One-sided: We are only interested in increasing platelet accumulation.
- Null hypothesis: The amount of platelets is the same before and after smoking.
- Alternative hypothesis: The number of platelets is higher after smoking than before.

### b

- Paired: The height of each self-pollinated seedling corresponds to the height of the cross-pollinated 'partner'.
- One-sided: We are only interested if the plants grow bigger.
- Null hypothesis: There is no difference between cross-pollinated and self-pollinated plants.
Alternative hypothesis: There is a significant difference between the two groups.

### c

- Unpaired: We have two distinct groups.
- Two-sided: We are interested in any effect on blood pressure.
- Null hypothesis: There is no difference in blood pressure between the two groups.
- Alternative hypothesis: There is a difference between the two groups.
### d

- Unpaired: We have two distinct groups.
- Two-sided: We are interested in the number of iron forms.
- Null hypothesis: There is no difference in the amount of iron between the groups/forms.
- Alternative hypothesis: There is a difference between the groups.

## Problem 6.2

### a

These are paired samples. Measurements are taken at the same location with both gauges.

### b

It's a one-sided test because we are assuming that the values from gauge B are larger.

### c

```{r}
gauge_a <- c(120, 265, 157, 187, 219, 288, 156, 205, 163)
gauge_b <- c(127, 281, 160, 185, 220, 298, 167, 203, 171)

t.test(x=gauge_a, y=gauge_b, alternative="less", paired=TRUE, conf.level=0.95)
```

There is a statistically significant difference.

## Problem 6.3

### a

The samples are unpaired because we are comparing two different groups: males and females.

### b

- Null hypotheses: There is no difference in length between the two groups.
- Alternative hypothesis: There is a difference in length between the two groups.

### c

```{r}
male <- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112)
female <- c(110, 111, 107, 108, 110, 105, 107, 106, 111, 111)

t.test(x=male, y=female, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is a statistically significant difference.

### d

```{r}
wilcox.test(x=male, y=female, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is a statistically significant difference.

### e

The result of the Wilcoxon-test is more trustworthy because, unlike the t-test, it does not assume that the data are normally distributed and we cannot verify this condition in any way.

## Problem 6.4

### a

Unpaired test: We investigated the calorie content of two different groups.

### b

Two-sided: We are interested in any difference.

### c

- Null hypotheses: There is no difference between the two groups.
- Alternative hypothesis: There is a difference between the two groups.

### d

```{r}
beef <- c(186, 181, 176, 149, 184, 190, 158, 139, 175, 148, 152, 111, 141, 153, 190, 157, 131, 149, 135, 132)
poultry <- c(129, 132, 102, 106, 94, 102, 87, 99, 170, 113, 135, 142, 86, 143, 152, 146, 144)

mean_beef <- mean(beef)
mean_poultry <- mean(poultry)

cat(mean_beef, "vs", mean_poultry)
```

The calorie content of beef hot dogs seems to be much higher than that of poultry
hot dogs. The null hypothesis may be rejected

### e

Since there is no indication whether the data are normally distributed, we choose a Wilcoxon test as a precautionary measure.

### f

```{r}
wilcox.test(x=beef, y=poultry, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is a statistically significant difference.

## Problem 6.5

### a

```{r}
zh <- c(16.3, 12.7, 14.0, 53.3, 117, 62.6, 27.6)
bl <- c(10.4, 8.91, 11.7, 29.9, 46.3, 25.0, 29.4)

mean_zh <- mean(zh, na.rm=FALSE)
mean_bl <- mean(bl, na.rm=FALSE)

sd_zh <- sd(zh, na.rm=FALSE)
sd_bl <- sd(bl, na.rm=FALSE)

cat("ZH: Mean =", mean_zh, "and SD =", sd_zh, "\n")
cat("BL: Mean =", mean_bl, "and SD =", sd_bl)
```

### b

The samples are unpaired if we argue that the cities constitute the experimental units.

### c

- Null hypotheses: There is no difference between the two groups.
- Alternative hypothesis: There is a difference between the two groups.

### d

```{r}
t.test(x=zh, y=bl, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is not a statistically significant difference.

### e

$[-15.33677, 55.87677]$

### f

```{r}
wilcox.test(x=zh, y=bl, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is not a statistically significant difference.

## Problem 6.6

```{r}
mf <- read.csv("/home/nils/dev/mscids-notes/hs25/sa/data/husband_wife.csv")
diff <- mf$age.husband - mf$age.wife

boxplot(diff, col = "orange")
```

### a

#### I

It is a paired test. For each test unit (married couple) there are two associated measurements (age husband, age wife).

#### II

We are not sure whether the husbands are really older than their wives. It is simply our impression and not a fact. So perform do a two-sided test.

#### III

- Null hypotheses: There is no difference between the two groups.
- Alternative hypothesis: There is a difference between the two groups.

```{r}
t.test(x=mf$age.husband, y=mf$age.wife, alternative="two.sided", paired=TRUE, conf.level=0.95)
```

There is a statistically significant difference.

#### IV

```{r}
wilcox.test(x=mf$age.husband, y=mf$age.wife, alternative="two.sided", paired=TRUE, conf.level=0.95)
```

There is a statistically significant difference.

### b

#### I

It is an unpaired and a two-sided test.

#### II

- Null hypotheses: There is no difference between the two groups.
- Alternative hypothesis: There is a difference between the two groups.

### III

```{r}
t.test(x=mf$height.husband, y=mf$height.wife, , mu=13, alternative="two.sided", paired=FALSE, conf.level=0.95)
```

There is not a statistically significant difference.

## Problem 6.7

### a

The test is paired because we measure the temperature of the same patients both before and after treatment.

### b

One-sided: We are interested in its fever-lowering effect.

### c


- Null hypotheses: There is no difference between the two groups.
- Alternative hypothesis: There is a significant difference between the two groups.

### d

```{r}
t1 <- c(39.1, 39.3, 38.9, 40.6, 39.5, 38.4, 38.6, 39.0, 38.6, 39.2)
t2 <- c(38.1, 38.3, 38.8, 37.8, 38.2, 37.3, 37.6, 37.8, 37.4, 38.1)

t.test(x=t1, y=t2, alternative="greater", paired=TRUE, conf.level=0.95)
```

There is a statistically significant difference.

### e

```{r}
wilcox.test(x=t1, y=t2, alternative="greater", paired=TRUE, conf.level=0.95)
```

There is a statistically significant difference.

### f

The p-value of the Wilcoxon-test is greater than the p-value of the t-test. Since the Wilcoxon-test assumes less (no normal distribution) than the t-test, there is an additional uncertainty. The null hypothesis is less strongly rejected.

## Problem 6.8

### a

True

### b

True

### c

True

### d

True

### e

True

# Problems 7

## Problem 7.1

### a

```{r}
inc <- read.table("/home/nils/dev/mscids-notes/hs25/sa/data/income.dat", header=TRUE)
```

### b

```{r}
# Plot data
plot(inc$Educ, inc$Income2005)

# Add linear reg to plot
abline(lm(inc$Income2005 ~inc$Educ), col="red")
```

### c

```{r}
m <- lm(inc$Income2005 ~inc$Educ)
m$coefficients
```

- a: The regression line crosses the $y$ axis at the point $x$ = -40199.575.
- b: Fore one step at the direction $x$ (one year of education), we increase the salary by 6451.475.

## d

```{r}
cat("The correlaiton between the education and income is:", cor(inc$Educ, inc$Income2005))
```

The corralation value ist near to O. The data points correlate loosly.

## Problem 7.2

### a

```{r}
head(anscombe)
```

### b

```{r}
par(mfrow=c(2,2))
plot(anscombe$x1, anscombe$y1)
reg <- lm(anscombe$y1 ~ anscombe$x1)
abline(reg)
title("x1, y1")

plot(anscombe$x2, anscombe$y2)
reg <- lm(anscombe$y2 ~ anscombe$x2)
abline(reg)
title("x2, y2")

plot(anscombe$x3, anscombe$y3)
reg <- lm(anscombe$y3 ~ anscombe$x3)
abline(reg)
title("x3, y3")

plot(anscombe$x4, anscombe$y4)
reg <- lm(anscombe$y4 ~ anscombe$x4)
abline(reg)
title("x4, y4")
```

### c

```{r}
lm(y1 ~ x1, data = anscombe)

lm(y2 ~ x2, data = anscombe)

lm(y3 ~ x3, data = anscombe)

lm(y4 ~ x4, data = anscombe)
```

The model coefficients are almost identical.

### d

```{r}
cat("The correlaiton between the x1 and y1 is:", cor(anscombe$x1, anscombe$y1), "\n")

cat("The correlaiton between the x2 and y2 is:", cor(anscombe$x2, anscombe$y2), "\n")

cat("The correlaiton between the x3 and y3 is:", cor(anscombe$x3, anscombe$y3), "\n")

cat("The correlaiton between the x4 and y4 is:", cor(anscombe$x4, anscombe$y4))
```

The correlation value is almost identical.

### Problem 7.3

```{r}
install.packages("ISLR")
library(ISLR)
```

### a

```{r}
head(Auto)
```

### b/c

```{r}
plot(Auto$horsepower, Auto$mpg)
model <- lm(mpg ~ horsepower, data=Auto)
summary(model)
```

#### I

The fuel consumption depends on the horsepower.

#### II

The $y$ value at position $x$ = 0 value has no practical meaning here.

#### III

```{r}
confint(model)
```

The confidence interval indicates the most likely range of values.

#### IV

```{r}
summary(model)$r.squared
```

The $R^2$ value is 0.606. This indicates that the variability to 60 % is through the model.

### d

```{r}
plot(Auto$horsepower, Auto$mpg)
model <- lm(mpg ~ horsepower, data=Auto)
abline(model, col="red")
```

### Problem 7.4

### a

```{r}
# ?MASS::Boston
```

### b

```{r}
library(MASS)
colnames(Boston)
```

### c

```{r}
attach(Boston)
```

### d

#### I/II

```{r}
model <- lm(medv ~ lstat, data=Boston)
summary(model)
```

### e

```{r}
names(model)
```

### f

```{r}
coef(model)
```

At the data point $x$ = 0, we start at a value of 34.6. Fore every step in $x$ direction, we lose -0.95. The p-value for `lstat` is close to 0 and therefore highly significant.

### g

```{r}
confint(model)
```

The model shows that the true $x$ and $y$ values lies between this ranges.

### h

```{r}
par(mfrow=c(1,1))
plot(Boston$medv, Boston$lstat)
abline(model, col="red")
```

### i

```{r}
summary(model)$r.squared
```

The $R^2$-value is 0.5441, so about 54 % of the variability is explained by the model.

# Problems 8

## Problem 8.1

```{r}
Auto <- read.csv("/home/nils/dev/mscids-notes/hs25/sa/data/auto.csv")

# Read data
head(Auto)

# Remove var "name"
Auto_1 <- within(Auto, rm(name))
head(Auto_1)
```

### a

```{r}
pairs(Auto_1)
```

### b

```{r}
cor(Auto_1)
```

The scatter plot and correlation value show a high positive correlation between the `horsepower` and `displacement` variables.

### c

```{r}
model <- lm(mpg ~ ., data=Auto_1)
summary(model)
```

#### I

The variables predict the response variable, `mpg`, statistically.

#### II

It seems that the variables `horsepower` and `year` have the greatest impact on the response variable `mpg`.

#### III

The variable `year` indicates a high positive correlation with the response variable `mpg`.

### d

```{r}
model <- lm(mpg ~ weight * year, data=Auto_1)
summary(model)
```

## Problem 8.2

```{r}
boston <- read.csv("/home/nils/dev/mscids-notes/hs25/sa/data/boston.csv")
head(boston)
```

### a

```{r}
pairs(boston)
model <- lm(medv ~ lstat + age, data=boston)
summary(model)
```

- $\hat{\beta}_0$ = 33.22: In neighborhoods where there is no population of lower status and no units build before 1940, the medium value of houses is $ 33 220.
- $\hat{\beta}_1$ = −1.03: For each additional percent of population of lower status, the medium value decreases by $ 1030.
- $\hat{\beta}_2$ = 0.03: For each additional percent of units build before 1949, the medium value increases by $ 30.
- All p-values are significant (below the significance level of 5 %), so all esti mates individually contribute significantly to the model.
- The $R^2$ value is 0.5513, therefore about 55 % of the variation is explained by the model.
- The p-value of the F value is below the significance level and therefore significant. The null hypothesis is rejected.

### b

```{r}
model <- lm(medv ~ ., data=boston)
summary(model)
```

The p-value is almost 1, so not significant at all. But in the first model, the p-value is 0.005, which is significant. That means that the variable age must correlate strongly with other variables.

### c

The more variables you have the bigger the $R^2$ value. That means that the $R^2$ is not a good indicator to compare different models.

### d

```{r}
model <- lm(medv ~ lstat * age, data=boston)
summary(model)
```

- $\hat{\beta}_0$ = 36.10: In neighborhoods where there is no population of lower status and no units build before 1940, the medium value of houses is $ 36 100.
- $\hat{\beta}_1$ = −1.39: For each additional percent of population of lower status, the medium value decreases by $ 1930.
- $\hat{\beta}_2$ = −0.00072: For each additional percent of units build before 1949, the medium value decreases by $ 0.27. As you can imagine, this value is not significant, as you can see from the output.
- $\hat{\beta}_{12}$ = 0.004: This coefficient is somewhat difficult to interpret and we didn’t do it in class.
- Not all p-values are significant (below the significance level of 5 %) any-
more.
- The $R^2$ value is 0.56, therefore about 56 % of the variation is explained by the model.
- The p-value of the F value is below the significance level and therefore significant. The null hypothesis H0 is rejected.

## Problem 8.3

### a

```{r}
cs <- read.csv("/home/nils/dev/mscids-notes/hs25/sa/data/carseats.csv")
head(cs)
```

### b

```{r}
model <- lm(Sales ~ Price + Urban + US, data=cs)
summary(model)
```

### c

- According to the model, 13.04 this is the average sales figures in shops reached in rural areas outside the USA, with the price of child seats still being $0 (not very realis-tic).
- The coefficient −0.05 indicates that for an increase of one dollar, an average of 0.05 units of child seats are sold less.
- The coefficient −0.021 means that on average 0.021 less units are sold in urban areas compared to rural areas. However, the p value is very high, so
this is more of a random variation.
- The 1.2 coefficient means that 1.2 more units are sold within the US compared to shops outside the USA. Perhaps child seats are compulsory in the
USA.

### d

$$
\text{Sales} = \beta_0 + \beta_1 \cdot \text{Price} + \beta_2 \cdot \text{Urban} + \beta_3 \cdot \text{US}
$$

> Note: General model!

### e

For all except `Urban`.

### f

```{r}
model <- lm(Sales ~ Price + Urban, data=cs)
summary(model)
```

### g

The model `lm(Sales ~ Price + Urban + US, data=cs)` is a mutible regression while the smaller model `model <- lm(Sales ~ Price + Urban, data=cs)` is a simple liniear regression.

# Problems 10

## Problem 10.1

|   | Pop | Lib | NotLib |
|---|-----|-----|--------|
| A | 40% | 50% | 50%    |
| B | 25% | 60% | 40%    |
| C | 35% | 35% | 65%    |

$$
P(B|L) = \dfrac{P(L|B)P(B)}{P(L)}
$$

- $P(L|B) = 60\%$
- $P(B)$ = 25\%$
- $P(L) = [P(L∣A)⋅P(A)]+[P(L∣B)⋅P(B)]+[P(L∣C)⋅P(C)] = 0.20+0.15+0.1225=0.4725$

$$
P(B|L) = \dfrac{0.6 \cdot 0.25}{0.4725} \approx 0.3175 = 31.75\%
$$

## Problem 10.2

### a

|         | 1     | 2     | 3     | 4      |
|---------|-------|-------|-------|--------|
| Model A | 1/4   | 1/4   | 1/4   | 1/4    |
| Model B | 1/10  | 2/10  | 3/10  | 4/10   |
| Model C | 12/25 | 12/50 | 12/75 | 12/100 |

- Model A: Fairest model since all values have the same likelihood.
- Model B: Unfair model. Value 4 has the highest chance of appearing.
- Model C: Unfair model. Value 1 has the highest chance of appearing.

### b

First try: After 100 throws, the results seem evenly distributed and reflect the distribution from model A.
Second try: We can see that value 1 appears much more frequently than value 4. It resembles model C.

## Problem 10.4

Since we don't know whether it's heads or tails, we need to assume a priori a 50% probability for each outcome.

## Problem 10.5

|              | Ice Creame | Fruits | French Fries | Pop |
|--------------|------------|--------|--------------|-----|
| 1st graders  | 0.3        | 0.6    | 0.1          | 0.2 |
| 6th graders  | 0.6        | 0.3    | 0.1          | 0.2 |
| 11th graders | 0.3        | 0.1    | 0.6          | 0.6 |
| Overall      | 0.36       | 0.24   | 0.4          | 1   |
