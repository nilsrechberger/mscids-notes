---
title: Linear Algebra 01
---

# What is Linear Algebra

Linear algebra is the theory of linear mappings between vector spaces. Broadly, it's the theory of certain linear data transformations or manipulations. The main actors are vectors and matrices.

## Vectors

A vector $\overrightarrow{a}$ is data of a number (or magnitude) and a direction. A vector is defined only by its length and direction. Two arrows represent the same vector if one is a parallel translation (shift) of the other. A vector is determined by giving the initial and terminal points of a representative, also called the tail and head of the vector. A vector, such as $\overrightarrow{PQ}$, is defined by the arrow with initial point $P$ (tail) and terminal point $Q$ (head).

> Note: The position of the arrow is not relevant.

$$
\overrightarrow{a} = \begin{bmatrix}
1 \\
-3 \\
\end{bmatrix}
$$

### Sum of Vectors

The sum of two vectors is calculated component-wise. The summation of vectors is commutative and associative, includes a neutral element (the zero vector), and can be inverted by its negative vector to result in the zero vector.

$$
\overrightarrow{a} + \overrightarrow{b} = \begin{bmatrix}
1 \\
-3 \\
\end{bmatrix}
+ 
\begin{bmatrix}
4 \\
-1 \\
\end{bmatrix} 
=
\begin{bmatrix}
5 \\
-2 \\
\end{bmatrix}
$$

### Subtraction

The difference of two vectors $\overrightarrow{a}$ and $\overrightarrow{b}$ is the sum of $\overrightarrow{a}$ and the opposite of $\overrightarrow{b}$.

$$
\overrightarrow{a} - \overrightarrow{b} = \overrightarrow{a} + (-\overrightarrow{b})
$$

### Scalar Multiplication

Scalar multiplication of a vector involves multiplying each component of the vector by a scalar (a real number). This changes the magnitude of the vector, keeping its direction the same if the scalar is positive or reversing it if the scalar is negative. Scalar multiplication of vectors is associative and distributive and includes a neutral element (the scalar 1).

$$
\dfrac{1}{2}(\overrightarrow{a} + \overrightarrow{b})
= 
\dfrac{1}{2}(\begin{bmatrix}
2 \\
6 \\
\end{bmatrix}
+ 
\begin{bmatrix}
4 \\
2 \\
\end{bmatrix}) 
=
\begin{bmatrix}
3 \\
4 \\
\end{bmatrix}
$$

### Magniture (Norm)

The magnitude (or norm) of a vector $\overrightarrow{a}$ is its length, and it is denoted by $|\overrightarrow{a}|$.

$$
\lvert \vec{a} \rvert = \sqrt{a_x^2 + a_y^2}
$$

> Note: Length of vector is never negative: $\lvert \overrightarrow{a} \rvert \ge 0$.

### Zero Vector

The vector with a norm of zero is denoted by $\overrightarrow{0}$. The length of the zero vector is $0$.

$$
|\overrightarrow{0}| = 0
$$

> Note: A zero vector has no direction.

### Opposite Vector

The opposite vector of $\overrightarrow{a}$ is a vector with the same norm but the opposite direction. It is denoted as $-\overrightarrow{a}$. In a representative, the initial and terminal points are reversed.

> Note: $\overrightarrow{PQ} = -\overrightarrow{QP}$, but $\overrightarrow{PQ} \neq \overrightarrow{QP}$ 

### Unite Vector

A unit vector is a vector that has a magnitude of 1 and is often used to indicate direction.

$$
\lvert \overrightarrow{e} \rvert = 1
$$

### Collinearity

Two vectors $\overrightarrow{a}$ and $\overrightarrow{b}$ are called collinear or linearly dependent, if:

$$
\overrightarrow{b} = r \overrightarrow{a} \quad \text{for } r \in \mathbb{R}
$$
$$
s \overrightarrow{b} =  \overrightarrow{a} \quad \text{for } s \in \mathbb{R}
$$

More generally:
$$
r \overrightarrow{a} + s \overrightarrow{b} = \overrightarrow{0} \quad \text{for } r, s \in \mathbb{R} \quad \text{with } r \neq 0 \quad \text{or } s \neq 0
$$

This will be used to generalize definition of linear independence.

### Normalization

For every non-zero vector $\overrightarrow{a}$, there is exactly one unit vector ($\overrightarrow{e}_a$) with the same direction. This process is called normalizing a vector.

$$
\overrightarrow{e}_a = \dfrac{1}{\lvert \overrightarrow{a} \rvert}\overrightarrow{a}
$$

### Vectors between two points

$$
\overrightarrow{PQ}
=
\begin{bmatrix}
x_2 - x_1 \\
y_2 - y_1 \\
\end{bmatrix}
$$

## Cartesian Coordinate Systems

A Cartesian coordinate system is one in which the axes are perpendicular and the units on each axis have the same length. The unit vectors along the axes are typically denoted by $\overrightarrow{e}_x$ and $\overrightarrow{e}_y$. Every vector in this system can be represented as a linear combination of these unit vectors.

$$
\overrightarrow{a} = a_x\overrightarrow{e}_x + a_y\overrightarrow{e}_y
$$

> Note: $a_x$ and $a_y$ are called scalar components of $\overrightarrow{a}$.

## 3-D Space

Vectors in 3D space have an additional component to represent the third dimension.

$$
\overrightarrow{a} = \begin{bmatrix}
1 \\
-3 \\
2 \\
\end{bmatrix}
$$

> Note: The mathematical rules do not change by adding more dimensions.

# System of Linear Equations

A system of linear equations is a collection of two or more linear equations that contain the same variables. The solution to the system is the set of values for the variables that satisfy all equations simultaneously. A linear equation in two variables, $x$ and $y$, is an equation of the form:

$$
ax + bx = c \text{, where } a, b, c \in \mathbb{R}
$$

A system of linear equations with two variables can only have one of the following possible solutions:

1. Exactly one solution: Corresponds to a unique intersection point

2. No solution: Corresponds to mutually exclusive, parallel objects.

3. Infinitely many solutions: Corresponds to identical objects or objects intersecting along a higher-dimensional object.

## Non-Linear equations

- Product of two unknown variables: $x \cdot y + 1 = x$
- Trigonomical terms: $\3\sin x + y = 0$
- Fractions of unknown variables: $\dfrac{1}{x} + y = 1$

## Matrix Notifications

To generalize determination of solutions of linear equations, we can use the matrix notification.

$$
\begin{multline}
2x + 3y = 10 \\
x - y = -5
\end{multline}
$$

Short hand notation by writing down coefficients only.

$$
\begin{pmatrix}
2 & 3 & | & 10 \\
1 & -1 & | & -5
\end{pmatrix}
$$

> Note: Called coefficient matrix.

## Gaussian Elimination Method

Goal of Gaussian elimination method is to apply sequence of elementary row operations in order to transform augmented matrix of a system into a matrix in row echelon form.

> Note: Echelon row form is not unique, since interchanging rows is allowed.

### Backward Substitution

The system of linear equations is written in a special type of matrix. Solution are easy to determine with so-called backward substitution.

$$
\begin{pmatrix}
2 & 1 & 1 & | & 5 \\
0 & 8 & 3 & | & 14 \\
0 & 0 & 1 & | & 2
\end{pmatrix}
$$

### Pivot element

The pivot element is a key element chosen at each step of an algorithm used to solve a system of linear equations.

> Note: The pivot element must be non-zero.

### Types of Row Echelon Forms

1. In all rows and in all columns there is a pivot: System has unique solutions
2. In all rows there is a pivot, but not in all columns: Free variables, so system has infinitely many solutions.
3. Some row has no pivot: System is inconsistent (no solutions).

## Set of Solutions

Often a free parameter is denoted by $t$: $t = y$

$$
L = \left\{ \begin{bmatrix} 3 \\ 0 \\ 1 \end{bmatrix} + t \begin{bmatrix} -3 \\ 1 \\ 0 \end{bmatrix} \mid t \in \mathbb{R} \right\}
$$

> Note: $t$ vary over whole set $\mathbb{R}$, i.e., from $-\infty$ to $\infty$
