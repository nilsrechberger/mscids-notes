{
  "hash": "8fb197cee1e36466b5e09818d218683f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classical and Bayesian Statistics - Exercises\"\ntoc: true\nformat:\n  html: default\n  pdf:\n    include-in-header:\n      - text: |\n          \\newcommand{\\lt}{<}\n          \\newcommand{\\gt}{>}\n---\n\n\n\n\n# Problems 1\n\n## a\n\nIt seems that, in total, the Allied and Axis countries had about the same number of civilian deaths. However, if we compare the numbers per party member, we see that the Axis countries had a higher average amount of deaths. On the other hand, the data from the Allies is inconsistent (see Denmark).\n\n## b\n\nThe percentages in the chart do not sum up to 100%.\n\n## c\n\n### I\n\nWe can assume that Linda is a bank teller and is active in the feminist movement. Her experiences from the past could have influenced her behavior and thinking on certain topics. She may have a sense for justice and equality, regardless of the topic.\n\n### II\n\nWe can assume that Steve is a librarian because of his helpful personality and his need for order and structure, much like what's found in a library. His passion for details may also be connected to a desire for knowledge.\n\n### III\n\nA Ball costs $0.05.\n\n### IV\n\nThere are more death by heart diseases than accidents.\n\n## d\n\nSince the engine is the only thing that keeps a plane in the air, it makes sense for it to be more armoured than the rest of the plane. Even if other parts have more bullet holes on average (e.g. the fuselage), the plane could still fly.\n\n# Problem 1.2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define vectors\nwinner <- c(193, 183, 191, 185, 185, 182, 182, 188, 188, 188, 185, 185, 177,\n182, 182, 193, 183, 179, 179, 175)\n\nopponent <- c(163, 191, 165, 187, 175, 193, 185, 187, 188, 173, 180, 177, 183,\n185, 180, 180, 182, 178, 178, 173)\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Determine length\ncat(\"Length of vector winners =\", length(winner), \"\\n\") # Add line break\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLength of vector winners = 20 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Length of vector opponent =\", length(opponent))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLength of vector opponent = 20\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Entries 6 to 10 =\",winner[6:10]) # Index starts at 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEntries 6 to 10 = 182 182 188 188 188\n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Some values from winner:\", winner[c(3, 5, 10, 12)]) # Passing a vector for selection\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSome values from winner: 191 185 188 185\n```\n\n\n:::\n:::\n\n\n\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Current values:\",winner[c(8, 9)], \"\\n\")  # Check values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent values: 188 188 \n```\n\n\n:::\n\n```{.r .cell-code}\nwinner[c(8, 9)] <- 189  # Reassign \ncat(\"New values:\", winner[c(8, 9)])  # Check values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNew values: 189 189\n```\n\n\n:::\n:::\n\n\n\n\n### e\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_winner <- mean(winner)\nmu_opponent <- mean(opponent)\n\ncat(\"Mean higth of winner vs. opponent:\", mu_winner, \"vs.\", mu_opponent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean higth of winner vs. opponent: 184.35 vs. 180.15\n```\n\n\n:::\n:::\n\n\n\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_diff <- mu_winner - mu_opponent\n\ncat(\"Differences between means =\", mu_diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifferences between means = 4.2\n```\n\n\n:::\n:::\n\n\n\n\n### g\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_winner <- var(winner)\nsd_winner <- sd(winner)\n\ncat(\"Variance / Std. deviation of winner:\", var_winner, \"/\", sd_winner)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance / Std. deviation of winner: 25.08158 / 5.008151\n```\n\n\n:::\n:::\n\n\n\n\n### h\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_variance <- function(data){\n    mu <- mean(data)\n    sum_of_squares <- sum((data - mu)^2)\n    variance <- sum_of_squares / (length(data) - 1)\n    \n    return(variance)\n}\n\nmy_stdDeviation <- function(variance){\n    stdDevition <- sqrt(variance)\n\n    return(stdDevition)\n}\n\nmy_var_winner <- my_variance(winner)\nmy_sd_winner <- my_stdDeviation(my_var_winner)\n\ncat(\"Variance of Winner =\", my_var_winner, \"\\n\") # Add line break\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance of Winner = 25.08158 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Variance of Winner =\", my_sd_winner)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance of Winner = 5.008151\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 1.3\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrades <- c(4.2, 2.3, 5.6, 4.5, 4.8, 3.9, 5.9, 2.4, 5.9, 6, 4, 3.7, 5, 5.2, 4.5, 3.6, 5, 6, 2.8, 3.3, 5.5, 4.2, 4.9, 5.1)\n\ngrades <- sort(grades) # Sort values\n\noriginal_mu <- mean(grades)\noriginla_meadian <- median(grades)\n\ngrades[9:11] <- 1 # Reassign values\nnew_mu <- mean(grades)\nnew_median <- median(grades)\n\ncat(\"Original vs. New mean:\", original_mu, \"vs.\", new_mu, \"\\n\") # Add line break\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal vs. New mean: 4.5125 vs. 4.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Median stays the same:\", originla_meadian, \"=\", new_median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedian stays the same: 4.65 = 4.65\n```\n\n\n:::\n:::\n\n\n\n\n# Problems 2\n\n## Probmel 2.1\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read data\ndata <- read.csv('/home/nils/dev/mscids-notes/hs25/sa/data/husband_wife.csv')\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age.husband height.husband age.wife height.wife\n1          49            180       43         159\n2          25            184       28         156\n3          40            165       30         162\n4          52            177       57         154\n5          58            161       52         142\n6          32            169       27         166\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age.husband    height.husband     age.wife      height.wife   \n Min.   :20.00   Min.   :155.0   Min.   :18.00   Min.   :141.0  \n 1st Qu.:33.00   1st Qu.:169.0   1st Qu.:32.00   1st Qu.:156.0  \n Median :43.50   Median :172.0   Median :41.00   Median :160.0  \n Mean   :42.92   Mean   :172.8   Mean   :40.68   Mean   :160.3  \n 3rd Qu.:53.00   3rd Qu.:177.0   3rd Qu.:50.00   3rd Qu.:165.0  \n Max.   :64.00   Max.   :190.0   Max.   :64.00   Max.   :176.0  \n```\n\n\n:::\n:::\n\n\n\n\nFor each column, we see a brief summary with quantitative and qualitative information about the data.\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_diff <- data$age.husband - data$age.wife # Calc age difference\n\nboxplot(age_diff)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n### d\n\n- The median of age_diff is about 2.5. On average, the age difference between husbands and wives is around 2.5 years.\n- 50% of the differences lie between approximately 0 and 5 years.\n- There are more upper than lower outliers, meaning that extreme cases where the husband is much older than the wife occur more frequently.\n- In addition, the values of the upper outliers are larger than those of the lower ones.\n\n## Problem 2.2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(InsectSprays) # Preview data from head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  count spray\n1    10     A\n2     7     A\n3    20     A\n4    14     A\n5    14     A\n6    12     A\n```\n\n\n:::\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(InsectSprays$count, InsectSprays$spray, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D         E         F \n14.500000 15.333333  2.083333  4.916667  3.500000 16.666667 \n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(count ~ spray,\n        data = InsectSprays)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n## Problem 2.3\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv('/home/nils/dev/mscids-notes/hs25/sa/data/Diet.csv')\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Person gender Age Height pre.weight Diet weight6weeks\n1     25     NA  41    171         60    2         60.0\n2     26     NA  32    174        103    2        103.0\n3      1      0  22    159         58    1         54.2\n4      2      0  46    192         60    1         54.0\n5      3      0  55    170         64    1         63.3\n6      4      0  33    171         64    1         61.1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add column weight.los\ndata$weight.loss <- data$weight6weeks - data$pre.weight\ndata$weight.loss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0.0  0.0 -3.8 -6.0 -0.7 -2.9 -2.8 -2.0 -2.0 -8.5 -1.9 -3.1 -1.5 -3.0 -3.6\n[16] -0.9  2.1 -2.0 -1.7 -4.3 -7.0 -0.6 -2.7 -3.6 -3.0 -2.0 -4.2 -4.7 -3.3  0.5\n[31] -7.0 -5.6 -3.4 -6.8 -7.8 -5.4 -6.8 -7.2 -7.0 -7.3 -0.9 -7.6 -4.1 -6.3 -5.0\n[46]  0.6 -1.1 -4.5 -4.1 -9.0 -2.4 -3.9 -3.5 -5.1 -3.5 -4.2 -2.4 -5.8 -3.5 -5.3\n[61] -1.7 -5.4 -6.1 -7.9  1.4 -4.3 -2.5 -0.9 -3.5 -0.5 -2.8 -8.6 -4.5 -2.8 -4.1\n[76] -5.3 -9.2 -6.1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(data$weight.loss, data$Diet, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3 \n-3.300000 -3.025926 -5.148148 \n```\n\n\n:::\n:::\n\n\n\n\nAccording to the data, diet 3 appears to have the greatest effect on weight loss over the 6-week therapy period. Diets 1 and 2 show more or less the same effect, although patients following diet 2 lost slightly less weight on average.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(data$weight.loss ~ data$Diet)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n- Even though diet 3 appears to have the greatest effect according to the median, it also has the largest interquartile range (IQR) among the three diets.\n- Diet 2 shows the greatest overall spread across the entire boxplot\n- Diet 1 is influenced by several lower outliers.\n\n## Problem 2.4\n\n### a\n\nThe probabilities of 'heads' and 'tails' do not add up to 1.\n\n### b\n\nThe calculated probability is negative. That's not possible by definition.\n\n### c\n\nThe union of the quantities `S` and `M` cannot be 0.7, because men cannot be pregnant.\n\n## Problem 2.5\n\n### a\n\nSample space of the experiment:\n\n$$\n\\Omega = \\{ (i, j) \\mid i, j \\in \\{1,2,3,4,5,6\\} \\}\n$$\n\n### b\n\n$$\np(\\omega_n) = \\dfrac{1}{36} = 0.02\\bar{7}\n$$\n\n### c\n\nEvents, where the sum is 7:\n\n$$\nE_1 = {(1, 6), (2, 5), (3, 4)}\n$$\n\n> Note: Since there are two dices, we can multiply the number of favourable results by 2.\n\nNow, we can calculate the probability:\n\n$$\np(E_1) = \\dfrac{6}{36} = 0.1\\bar{6}\n$$\n\n### d\n\n$$\nE_2 = {(1, 1), (1, 2), (2, 1)}\n$$\n\n$$\np(E_2) = \\dfrac{3}{36} = 0.08\\bar{3}\n$$\n\n### e\n\n$$\nE_3 = \\{ (i, j) \\mid i, j \\in \\{1,3,5\\} \\}\n$$\n\n$$\np(E_3) = \\dfrac{9}{36} = 0.25\n$$\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_e2 <- 3/36\np_e3 <- 9/36\np_intersection <- 1/36\n\np_annual <- p_e2 + p_e3 - p_intersection\n\nprint(p_annual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3055556\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 2.6\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_A <- 3/4\np_B <- 2/3\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_bothEvents <- p_A * p_B\nprint(p_bothEvents)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_atLeastOne <- p_A + p_B - p_A * p_B\nprint(p_atLeastOne)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9166667\n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_atMostOne <- 1 - p_A * p_B\n\ncat(p_atMostOne)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5\n```\n\n\n:::\n:::\n\n\n\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_noEvent <- 1 - (p_A + p_B - p_A * p_B)\nprint(p_noEvent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08333333\n```\n\n\n:::\n:::\n\n\n\n\n### e\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_exactlyOneEvent <- p_A + p_B - 2 * p_A * p_B\nprint(p_exactlyOneEvent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4166667\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 2.7\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_earthquake <- 0.04\np_typhoon <- 0.08\n\np_annual <- p_earthquake + p_typhoon - p_earthquake * p_typhoon\n\nprint(p_annual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1168\n```\n\n\n:::\n:::\n\n\n\n\n# Problems 3\n\n## Problem 3.1\n\n$$\np_2 = 1 - 0.3 - 0.1 - 0.2 - 0.3 = 0.1\n$$\n\n## Problem 3.2\n\n### a\n\nThe probabilities in the table sum to one, so it is a probability distribution.\n\n$$\n\\sum P(X = k) = 1\n$$\n\n### b\n\n$$\np(2 \\le k \\le 4) = 0.2 + 0.2 + 0.1 = 0.5\n$$\n\n### c\n\n$$\np(k \\gt 2) = 0.2 + 0.1 + 0.1 = 0.4\n$$\n\n\n### d\n\n$$\np(k \\le 4) = 1 - 0.1 = 0.9\n$$\n\n\n### e\n\n$$\np(k \\gt 1) = 1 - 0.4 = 0.6\n$$\n\n\n## Problem 3.3\n\n### a\n\n$$\np(k \\le 13) = 0.992\n$$\n\n### b\n\n$$\np(k \\ge 10) = 1 - 0.939 = 0.061\n$$\n\n### c\n\n$$\np(k = 15) = 1 - 0.999 = 0.001\n$$\n\n### d\n\n$$\np(9 \\le k \\le 12) = 0.989 - 0.711 = 0.282\n$$\n\n## Problem 3.4\n\n### a\n\n$$\n\\Omega = \\{\\text{TTT}, \\text{TTH}, \\text{THT}, \\text{HTT}, \\text{THH}, \\text{HTH}, \\text{HHT}, \\text{HHH}\\}\n$$\n$$\nP(X = 0) = \\frac{1}{8}\n$$\n$$\nP(X = 1) = \\frac{3}{8}\n$$\n$$\nP(X = 2) = \\frac{3}{8}\n$$\n$$\nP(X = 3) = \\frac{1}{8}\n$$\n\n### b\n\n$$\np(x = 2) = \\frac{3}{8}\n$$\n\n### c\n\n$$\np(X \\ge 2) = \\frac{3}{8} + \\frac{1}{8} = \\frac{1}{2}\n$$\n\n### d\n\n$$\np(X \\le 1) = \\frac{1}{8} + \\frac{3}{8} = \\frac{1}{2}\n$$\n\n## Problem 3.5\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_k <- c(-5, -4, 1, 3, 6)\np <- c(0.3, 0.1, 0.2, 0.3)\np_k <- 1 - sum(p) # Calc p_k\ncat(\"Probability of -4 =\", p_k, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProbability of -4 = 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\np <- c(0.3, p_k, 0.1, 0.2, 0.3) # Reassign p\n\nmu <- sum(x_k * p) # Calc expected value\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 3.6\n\n### a\n\n$$\np(x) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}\n$$\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 2:12 # Sum of eyes\np <- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36 # Probability of summed eyes\n\nmu = sum(x * p)\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n\n```{.r .cell-code}\nvar = sum((x - mu)**2 * p)\nvar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.833333\n```\n\n\n:::\n\n```{.r .cell-code}\nsd = sqrt(var)\nsd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.415229\n```\n\n\n:::\n:::\n\n\n\n\n# Problems 4\n\n## Problem 4.2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define distr. paras\nmu <- 4\nsd <- 1.25\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assume normal distr.\npnorm(q = 2.5, mean = mu, sd = sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1150697\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(q = 5.0, mean = mu, sd = sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2118554\n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(q = 4.5, mean = mu, sd = sd) - pnorm(q = 3.0, mean = mu, sd = sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4435663\n```\n\n\n:::\n:::\n\n\n\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.98, mean=mu, sd=sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.567186\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 4.3\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define distr. paras\nmu <- 2.2\nsd <- 0.3\n\n# Assume normal distr.\n1 - pnorm(q=3.1, mean=mu, sd=sd/sqrt(100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 4.4\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define distr. paras\nmu <- 8.2\nsd <- 6.0\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assume normal distr.\npnorm(q=10.0, mean=mu, sd=sd/sqrt(36))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9640697\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(q=10.0, mean=mu, sd=sd/sqrt(36)) - pnorm(q=5.0, mean=mu, sd=sd/sqrt(36))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9633825\n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1- pnorm(q=20.0, mean=mu, sd=sd/sqrt(36))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n### d\n\nIt's small, but not impossible. We also assume a normal distribution. The real distribution probability may differ from the normal distribution. We also use a very small sample size of 36.\n\n### e\n\nYes, the i.i.d. assumption holds here because each of the 36 passengers is an individual who is independent of the others.\n\n## Problem 4.5\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define distr. paras\nmu <- 77\nsd <- 15\n\ncourse_1 <- 25\ncourse_2 <- 64\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assume normal distr.\npnorm(q=82, mean=mu, sd=sd/sqrt(course_1)) - pnorm(q=72, mean=mu, sd=sd/sqrt(course_1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9044193\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(q=82, mean=mu, sd=sd/sqrt(course_2)) - pnorm(q=72, mean=mu, sd=sd/sqrt(course_2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9923392\n```\n\n\n:::\n:::\n\n\n\n\nFor a larger group, the probability is more likely to be at the mean compared to a smaller group (CLT).\n\n# Problems 5\n\n## Problem 5.1\n\n- $H_0$: $\\mu = \\mu_0 = 70$\n- $H_A$: $\\mu \\lt 70$\n\nRejection range:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(71, 69, 67, 68, 73, 72, 71, 71, 68, 72, 69, 72)\nsd <- 1.5\n\nmu_hat <- mean(data)\n\nqnorm(p = 0.05, mean = 70, sd = 1.5/sqrt(12))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 69.28776\n```\n\n\n:::\n:::\n\n\n\n\nTest:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(q = mu_hat, mean = 70, sd = 1.5 / sqrt(12))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7181486\n```\n\n\n:::\n:::\n\n\n\n\nWe do not reject the null hypothesis.\n\n- p-value: 0.718\n\nThe mean of the sample does not statistical deviate from the producers claimed mean.\n\n## Problem 5.2\n\n### a\n\n- $H_0$: $\\mu = \\mu_0 = 50$\n- $H_A$: $\\mu \\lt 50$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(46, 48, 52, 49, 46, 51, 52, 47, 49, 44, 48, 51, 49, 50, 53, 47)\nsd <- 3.0\n\nmu_hat <- mean(data)\n\npnorm(q = mu_hat, mean = 50, sd = 3.0/sqrt(16))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0668072\n```\n\n\n:::\n:::\n\n\n\n\nWe do not reject the null hypothesis.\n\n- p-value: 0.0668072\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(46, 48, 52, 49, 46, 51, 52, 47, 49, 44, 48, 51, 49, 50, 53, 47)\nsd <- 3.0\n\nmu_hat <- mean(data)\n\npnorm(q = mu_hat, mean = 50, sd = 3.0/sqrt(100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.841729e-05\n```\n\n\n:::\n:::\n\n\n\n\nWe reject the null hypothesis.\n\n# Problems 6\n\n## Problem 6.1\n\n### a\n\n- Paired samples: We use the same people for the before and after smoking measurements.\n- One-sided: We are only interested in increasing platelet accumulation.\n- Null hypothesis: The amount of platelets is the same before and after smoking.\n- Alternative hypothesis: The number of platelets is higher after smoking than before.\n\n### b\n\n- Paired: The height of each self-pollinated seedling corresponds to the height of the cross-pollinated 'partner'.\n- One-sided: We are only interested if the plants grow bigger.\n- Null hypothesis: There is no difference between cross-pollinated and self-pollinated plants.\nAlternative hypothesis: There is a significant difference between the two groups.\n\n### c\n\n- Unpaired: We have two distinct groups.\n- Two-sided: We are interested in any effect on blood pressure.\n- Null hypothesis: There is no difference in blood pressure between the two groups.\n- Alternative hypothesis: There is a difference between the two groups.\n### d\n\n- Unpaired: We have two distinct groups.\n- Two-sided: We are interested in the number of iron forms.\n- Null hypothesis: There is no difference in the amount of iron between the groups/forms.\n- Alternative hypothesis: There is a difference between the groups.\n\n## Problem 6.2\n\n### a\n\nThese are paired samples. Measurements are taken at the same location with both gauges.\n\n### b\n\nIt's a one-sided test because we are assuming that the values from gauge B are larger.\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngauge_a <- c(120, 265, 157, 187, 219, 288, 156, 205, 163)\ngauge_b <- c(127, 281, 160, 185, 220, 298, 167, 203, 171)\n\nt.test(x=gauge_a, y=gauge_b, alternative=\"less\", paired=TRUE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  gauge_a and gauge_b\nt = -2.7955, df = 8, p-value = 0.01168\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n     -Inf -1.93449\nsample estimates:\nmean difference \n      -5.777778 \n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n## Problem 6.3\n\n### a\n\nThe samples are unpaired because we are comparing two different groups: males and females.\n\n### b\n\n- Null hypotheses: There is no difference in length between the two groups.\n- Alternative hypothesis: There is a difference in length between the two groups.\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmale <- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112)\nfemale <- c(110, 111, 107, 108, 110, 105, 107, 106, 111, 111)\n\nt.test(x=male, y=female, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  male and female\nt = 3.4843, df = 14.894, p-value = 0.00336\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.861895 7.738105\nsample estimates:\nmean of x mean of y \n    113.4     108.6 \n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x=male, y=female, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x = male, y = female, alternative = \"two.sided\",\n: cannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  male and female\nW = 87.5, p-value = 0.004845\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n### e\n\nThe result of the Wilcoxon-test is more trustworthy because, unlike the t-test, it does not assume that the data are normally distributed and we cannot verify this condition in any way.\n\n## Problem 6.4\n\n### a\n\nUnpaired test: We investigated the calorie content of two different groups.\n\n### b\n\nTwo-sided: We are interested in any difference.\n\n### c\n\n- Null hypotheses: There is no difference between the two groups.\n- Alternative hypothesis: There is a difference between the two groups.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeef <- c(186, 181, 176, 149, 184, 190, 158, 139, 175, 148, 152, 111, 141, 153, 190, 157, 131, 149, 135, 132)\npoultry <- c(129, 132, 102, 106, 94, 102, 87, 99, 170, 113, 135, 142, 86, 143, 152, 146, 144)\n\nmean_beef <- mean(beef)\nmean_poultry <- mean(poultry)\n\ncat(mean_beef, \"vs\", mean_poultry)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n156.85 vs 122.4706\n```\n\n\n:::\n:::\n\n\n\n\nThe calorie content of beef hot dogs seems to be much higher than that of poultry\nhot dogs. The null hypothesis may be rejected\n\n### e\n\nSince there is no indication whether the data are normally distributed, we choose a Wilcoxon test as a precautionary measure.\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x=beef, y=poultry, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x = beef, y = poultry, alternative =\n\"two.sided\", : cannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  beef and poultry\nW = 285.5, p-value = 0.0004549\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n## Problem 6.5\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzh <- c(16.3, 12.7, 14.0, 53.3, 117, 62.6, 27.6)\nbl <- c(10.4, 8.91, 11.7, 29.9, 46.3, 25.0, 29.4)\n\nmean_zh <- mean(zh, na.rm=FALSE)\nmean_bl <- mean(bl, na.rm=FALSE)\n\nsd_zh <- sd(zh, na.rm=FALSE)\nsd_bl <- sd(bl, na.rm=FALSE)\n\ncat(\"ZH: Mean =\", mean_zh, \"and SD =\", sd_zh, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nZH: Mean = 43.35714 and SD = 38.02301 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"BL: Mean =\", mean_bl, \"and SD =\", sd_bl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBL: Mean = 23.08714 and SD = 13.66495\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\nThe samples are unpaired if we argue that the cities constitute the experimental units.\n\n### c\n\n- Null hypotheses: There is no difference between the two groups.\n- Alternative hypothesis: There is a difference between the two groups.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x=zh, y=bl, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  zh and bl\nt = 1.3273, df = 7.5245, p-value = 0.2233\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.33677  55.87677\nsample estimates:\nmean of x mean of y \n 43.35714  23.08714 \n```\n\n\n:::\n:::\n\n\n\n\nThere is not a statistically significant difference.\n\n### e\n\n$[-15.33677, 55.87677]$\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x=zh, y=bl, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum exact test\n\ndata:  zh and bl\nW = 34, p-value = 0.2593\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\nThere is not a statistically significant difference.\n\n## Problem 6.6\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmf <- read.csv(\"/home/nils/dev/mscids-notes/hs25/sa/data/husband_wife.csv\")\ndiff <- mf$age.husband - mf$age.wife\n\nboxplot(diff, col = \"orange\")\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\n\n\n### a\n\n#### I\n\nIt is a paired test. For each test unit (married couple) there are two associated measurements (age husband, age wife).\n\n#### II\n\nWe are not sure whether the husbands are really older than their wives. It is simply our impression and not a fact. So perform do a two-sided test.\n\n#### III\n\n- Null hypotheses: There is no difference between the two groups.\n- Alternative hypothesis: There is a difference between the two groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x=mf$age.husband, y=mf$age.wife, alternative=\"two.sided\", paired=TRUE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  mf$age.husband and mf$age.wife\nt = 7.1518, df = 169, p-value = 2.474e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.618286 2.852302\nsample estimates:\nmean difference \n       2.235294 \n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n#### IV\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x=mf$age.husband, y=mf$age.wife, alternative=\"two.sided\", paired=TRUE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  mf$age.husband and mf$age.wife\nV = 9460, p-value = 3.977e-12\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n### b\n\n#### I\n\nIt is an unpaired and a two-sided test.\n\n#### II\n\n- Null hypotheses: There is no difference between the two groups.\n- Alternative hypothesis: There is a difference between the two groups.\n\n### III\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x=mf$height.husband, y=mf$height.wife, , mu=13, alternative=\"two.sided\", paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  mf$height.husband and mf$height.wife\nt = -0.63293, df = 336.53, p-value = 0.5272\nalternative hypothesis: true difference in means is not equal to 13\n95 percent confidence interval:\n 11.18772 13.92993\nsample estimates:\nmean of x mean of y \n 172.8471  160.2882 \n```\n\n\n:::\n:::\n\n\n\n\nThere is not a statistically significant difference.\n\n## Problem 6.7\n\n### a\n\nThe test is paired because we measure the temperature of the same patients both before and after treatment.\n\n### b\n\nOne-sided: We are interested in its fever-lowering effect.\n\n### c\n\n\n- Null hypotheses: There is no difference between the two groups.\n- Alternative hypothesis: There is a significant difference between the two groups.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- c(39.1, 39.3, 38.9, 40.6, 39.5, 38.4, 38.6, 39.0, 38.6, 39.2)\nt2 <- c(38.1, 38.3, 38.8, 37.8, 38.2, 37.3, 37.6, 37.8, 37.4, 38.1)\n\nt.test(x=t1, y=t2, alternative=\"greater\", paired=TRUE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  t1 and t2\nt = 5.6569, df = 9, p-value = 0.0001554\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.7976252       Inf\nsample estimates:\nmean difference \n           1.18 \n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n### e\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(x=t1, y=t2, alternative=\"greater\", paired=TRUE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x = t1, y = t2, alternative = \"greater\", :\ncannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  t1 and t2\nV = 55, p-value = 0.002865\nalternative hypothesis: true location shift is greater than 0\n```\n\n\n:::\n:::\n\n\n\n\nThere is a statistically significant difference.\n\n### f\n\nThe p-value of the Wilcoxon-test is greater than the p-value of the t-test. Since the Wilcoxon-test assumes less (no normal distribution) than the t-test, there is an additional uncertainty. The null hypothesis is less strongly rejected.\n\n## Problem 6.8\n\n### a\n\nTrue\n\n### b\n\nTrue\n\n### c\n\nTrue\n\n### d\n\nTrue\n\n### e\n\nTrue\n\n# Problems 7\n\n## Problem 7.1\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc <- read.table(\"/home/nils/dev/mscids-notes/hs25/sa/data/income.dat\", header=TRUE)\n```\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot data\nplot(inc$Educ, inc$Income2005)\n\n# Add linear reg to plot\nabline(lm(inc$Income2005 ~inc$Educ), col=\"red\")\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(inc$Income2005 ~inc$Educ)\nm$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    inc$Educ \n -40199.575    6451.475 \n```\n\n\n:::\n:::\n\n\n\n\n- a: The regression line crosses the $y$ axis at the point $x$ = -40199.575.\n- b: Fore one step at the direction $x$ (one year of education), we increase the salary by 6451.475.\n\n## d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"The correlaiton between the education and income is:\", cor(inc$Educ, inc$Income2005))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe correlaiton between the education and income is: 0.3456474\n```\n\n\n:::\n:::\n\n\n\n\nThe corralation value ist near to O. The data points correlate loosly.\n\n## Problem 7.2\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(anscombe$x1, anscombe$y1)\nreg <- lm(anscombe$y1 ~ anscombe$x1)\nabline(reg)\ntitle(\"x1, y1\")\n\nplot(anscombe$x2, anscombe$y2)\nreg <- lm(anscombe$y2 ~ anscombe$x2)\nabline(reg)\ntitle(\"x2, y2\")\n\nplot(anscombe$x3, anscombe$y3)\nreg <- lm(anscombe$y3 ~ anscombe$x3)\nabline(reg)\ntitle(\"x3, y3\")\n\nplot(anscombe$x4, anscombe$y4)\nreg <- lm(anscombe$y4 ~ anscombe$x4)\nabline(reg)\ntitle(\"x4, y4\")\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y1 ~ x1, data = anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nCoefficients:\n(Intercept)           x1  \n     3.0001       0.5001  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(y2 ~ x2, data = anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nCoefficients:\n(Intercept)           x2  \n      3.001        0.500  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(y3 ~ x3, data = anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nCoefficients:\n(Intercept)           x3  \n     3.0025       0.4997  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(y4 ~ x4, data = anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nCoefficients:\n(Intercept)           x4  \n     3.0017       0.4999  \n```\n\n\n:::\n:::\n\n\n\n\nThe model coefficients are almost identical.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"The correlaiton between the x1 and y1 is:\", cor(anscombe$x1, anscombe$y1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe correlaiton between the x1 and y1 is: 0.8164205 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"The correlaiton between the x2 and y2 is:\", cor(anscombe$x2, anscombe$y2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe correlaiton between the x2 and y2 is: 0.8162365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"The correlaiton between the x3 and y3 is:\", cor(anscombe$x3, anscombe$y3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe correlaiton between the x3 and y3 is: 0.8162867 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"The correlaiton between the x4 and y4 is:\", cor(anscombe$x4, anscombe$y4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe correlaiton between the x4 and y4 is: 0.8165214\n```\n\n\n:::\n:::\n\n\n\n\nThe correlation value is almost identical.\n\n### Problem 7.3\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ISLR\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nInstalling package into '/home/nils/R/x86_64-pc-linux-gnu-library/4.5'\n(as 'lib' is unspecified)\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ISLR)\n```\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Auto)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n5  17         8          302        140   3449         10.5   70      1\n6  15         8          429        198   4341         10.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n5               ford torino\n6          ford galaxie 500\n```\n\n\n:::\n:::\n\n\n\n\n### b/c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Auto$horsepower, Auto$mpg)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ horsepower, data=Auto)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,\tAdjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n#### I\n\nThe fuel consumption depends on the horsepower.\n\n#### II\n\nThe $y$ value at position $x$ = 0 value has no practical meaning here.\n\n#### III\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %     97.5 %\n(Intercept) 38.525212 41.3465103\nhorsepower  -0.170517 -0.1451725\n```\n\n\n:::\n:::\n\n\n\n\nThe confidence interval indicates the most likely range of values.\n\n#### IV\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6059483\n```\n\n\n:::\n:::\n\n\n\n\nThe $R^2$ value is 0.606. This indicates that the variability to 60 % is through the model.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Auto$horsepower, Auto$mpg)\nmodel <- lm(mpg ~ horsepower, data=Auto)\nabline(model, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-75-1.png){width=672}\n:::\n:::\n\n\n\n\n### Problem 7.4\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ?MASS::Boston\n```\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\ncolnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Boston)\n```\n:::\n\n\n\n\n### d\n\n#### I/II\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(medv ~ lstat, data=Boston)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,\tAdjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n### e\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n\n\n:::\n:::\n\n\n\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n```\n\n\n:::\n:::\n\n\n\n\nAt the data point $x$ = 0, we start at a value of 34.6. Fore every step in $x$ direction, we lose -0.95. The p-value for `lstat` is close to 0 and therefore highly significant.\n\n### g\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n```\n\n\n:::\n:::\n\n\n\n\nThe model shows that the true $x$ and $y$ values lies between this ranges.\n\n### h\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\nplot(Boston$medv, Boston$lstat)\nabline(model, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-83-1.png){width=672}\n:::\n:::\n\n\n\n\n### i\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5441463\n```\n\n\n:::\n:::\n\n\n\n\nThe $R^2$-value is 0.5441, so about 54 % of the variability is explained by the model.\n\n# Problems 8\n\n## Problem 8.1\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAuto <- read.csv(\"/home/nils/dev/mscids-notes/hs25/sa/data/auto.csv\")\n\n# Read data\nhead(Auto)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X mpg cylinders displacement horsepower weight acceleration year origin\n1 1  18         8          307        130   3504         12.0   70      1\n2 2  15         8          350        165   3693         11.5   70      1\n3 3  18         8          318        150   3436         11.0   70      1\n4 4  16         8          304        150   3433         12.0   70      1\n5 5  17         8          302        140   3449         10.5   70      1\n6 6  15         8          429        198   4341         10.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n5               ford torino\n6          ford galaxie 500\n```\n\n\n:::\n\n```{.r .cell-code}\n# Remove var \"name\"\nAuto_1 <- within(Auto, rm(name))\nhead(Auto_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X mpg cylinders displacement horsepower weight acceleration year origin\n1 1  18         8          307        130   3504         12.0   70      1\n2 2  15         8          350        165   3693         11.5   70      1\n3 3  18         8          318        150   3436         11.0   70      1\n4 4  16         8          304        150   3433         12.0   70      1\n5 5  17         8          302        140   3449         10.5   70      1\n6 6  15         8          429        198   4341         10.0   70      1\n```\n\n\n:::\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(Auto_1)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-86-1.png){width=672}\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(Auto_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      X        mpg  cylinders displacement horsepower\nX             1.0000000  0.5863298 -0.3602752   -0.3871458 -0.4229250\nmpg           0.5863298  1.0000000 -0.7776175   -0.8051269 -0.7784268\ncylinders    -0.3602752 -0.7776175  1.0000000    0.9508233  0.8429834\ndisplacement -0.3871458 -0.8051269  0.9508233    1.0000000  0.8972570\nhorsepower   -0.4229250 -0.7784268  0.8429834    0.8972570  1.0000000\nweight       -0.3217474 -0.8322442  0.8975273    0.9329944  0.8645377\nacceleration  0.2909849  0.4233285 -0.5046834   -0.5438005 -0.6891955\nyear          0.9967805  0.5805410 -0.3456474   -0.3698552 -0.4163615\norigin        0.2005760  0.5652088 -0.5689316   -0.6145351 -0.4551715\n                 weight acceleration       year     origin\nX            -0.3217474    0.2909849  0.9967805  0.2005760\nmpg          -0.8322442    0.4233285  0.5805410  0.5652088\ncylinders     0.8975273   -0.5046834 -0.3456474 -0.5689316\ndisplacement  0.9329944   -0.5438005 -0.3698552 -0.6145351\nhorsepower    0.8645377   -0.6891955 -0.4163615 -0.4551715\nweight        1.0000000   -0.4168392 -0.3091199 -0.5850054\nacceleration -0.4168392    1.0000000  0.2903161  0.2127458\nyear         -0.3091199    0.2903161  1.0000000  0.1815277\norigin       -0.5850054    0.2127458  0.1815277  1.0000000\n```\n\n\n:::\n:::\n\n\n\n\nThe scatter plot and correlation value show a high positive correlation between the `horsepower` and `displacement` variables.\n\n### c\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ ., data=Auto_1)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ ., data = Auto_1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6234 -2.1948 -0.1499  1.8294 12.9947 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.800e+01  4.259e+01  -1.831  0.06785 .  \nX            -2.829e-02  1.971e-02  -1.436  0.15196    \ncylinders    -4.564e-01  3.239e-01  -1.409  0.15956    \ndisplacement  1.715e-02  7.744e-03   2.215  0.02735 *  \nhorsepower   -1.431e-02  1.389e-02  -1.030  0.30348    \nweight       -6.378e-03  6.546e-04  -9.745  < 2e-16 ***\nacceleration  7.505e-02  9.878e-02   0.760  0.44790    \nyear          1.622e+00  6.092e-01   2.663  0.00807 ** \norigin        1.455e+00  2.785e-01   5.224 2.88e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.323 on 383 degrees of freedom\nMultiple R-squared:  0.8224,\tAdjusted R-squared:  0.8187 \nF-statistic: 221.7 on 8 and 383 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n#### I\n\nThe variables predict the response variable, `mpg`, statistically.\n\n#### II\n\nIt seems that the variables `horsepower` and `year` have the greatest impact on the response variable `mpg`.\n\n#### III\n\nThe variable `year` indicates a high positive correlation with the response variable `mpg`.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ weight * year, data=Auto_1)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ weight * year, data = Auto_1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0397 -1.9956 -0.0983  1.6525 12.9896 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.105e+02  1.295e+01  -8.531 3.30e-16 ***\nweight       2.755e-02  4.413e-03   6.242 1.14e-09 ***\nyear         2.040e+00  1.718e-01  11.876  < 2e-16 ***\nweight:year -4.579e-04  5.907e-05  -7.752 8.02e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.193 on 388 degrees of freedom\nMultiple R-squared:  0.8339,\tAdjusted R-squared:  0.8326 \nF-statistic: 649.3 on 3 and 388 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Problem 8.2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston <- read.csv(\"/home/nils/dev/mscids-notes/hs25/sa/data/boston.csv\")\nhead(boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X    crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n```\n\n\n:::\n:::\n\n\n\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(boston)\n```\n\n::: {.cell-output-display}\n![](sa_exercises_files/figure-html/unnamed-chunk-91-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- lm(medv ~ lstat + age, data=boston)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ lstat + age, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,\tAdjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n- $\\hat{\\beta}_0$ = 33.22: In neighborhoods where there is no population of lower status and no units build before 1940, the medium value of houses is $ 33 220.\n- $\\hat{\\beta}_1$ = 1.03: For each additional percent of population of lower status, the medium value decreases by $ 1030.\n- $\\hat{\\beta}_2$ = 0.03: For each additional percent of units build before 1949, the medium value increases by $ 30.\n- All p-values are significant (below the significance level of 5 %), so all esti mates individually contribute significantly to the model.\n- The $R^2$ value is 0.5513, therefore about 55 % of the variation is explained by the model.\n- The p-value of the F value is below the significance level and therefore significant. The null hypothesis is rejected.\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(medv ~ ., data=boston)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ ., data = boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.8948  -2.7585  -0.4663   1.7963  26.0911 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.461352   5.100994   7.148 3.21e-12 ***\nX            -0.002526   0.002080  -1.215 0.225046    \ncrim         -0.108762   0.032855  -3.310 0.001000 ** \nzn            0.048031   0.013785   3.484 0.000538 ***\nindus         0.019932   0.061468   0.324 0.745871    \nchas          2.705245   0.861298   3.141 0.001786 ** \nnox         -17.541602   3.822390  -4.589 5.66e-06 ***\nrm            3.839225   0.418422   9.175  < 2e-16 ***\nage          -0.001938   0.013380  -0.145 0.884866    \ndis          -1.493304   0.199892  -7.471 3.68e-13 ***\nrad           0.324925   0.068111   4.771 2.43e-06 ***\ntax          -0.011598   0.003807  -3.046 0.002443 ** \nptratio      -0.947985   0.130822  -7.246 1.67e-12 ***\nblack         0.009357   0.002685   3.485 0.000536 ***\nlstat        -0.526184   0.050704 -10.377  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.743 on 491 degrees of freedom\nMultiple R-squared:  0.7414,\tAdjusted R-squared:  0.734 \nF-statistic: 100.6 on 14 and 491 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nThe p-value is almost 1, so not significant at all. But in the first model, the p-value is 0.005, which is significant. That means that the variable age must correlate strongly with other variables.\n\n### c\n\nThe more variables you have the bigger the $R^2$ value. That means that the $R^2$ is not a good indicator to compare different models.\n\n### d\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(medv ~ lstat * age, data=boston)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ lstat * age, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,\tAdjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n- $\\hat{\\beta}_0$ = 36.10: In neighborhoods where there is no population of lower status and no units build before 1940, the medium value of houses is $ 36 100.\n- $\\hat{\\beta}_1$ = 1.39: For each additional percent of population of lower status, the medium value decreases by $ 1930.\n- $\\hat{\\beta}_2$ = 0.00072: For each additional percent of units build before 1949, the medium value decreases by $ 0.27. As you can imagine, this value is not significant, as you can see from the output.\n- $\\hat{\\beta}_{12}$ = 0.004: This coefficient is somewhat difficult to interpret and we didnt do it in class.\n- Not all p-values are significant (below the significance level of 5 %) any-\nmore.\n- The $R^2$ value is 0.56, therefore about 56 % of the variation is explained by the model.\n- The p-value of the F value is below the significance level and therefore significant. The null hypothesis H0 is rejected.\n\n## Problem 8.3\n\n### a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs <- read.csv(\"/home/nils/dev/mscids-notes/hs25/sa/data/carseats.csv\")\nhead(cs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1 1  9.50       138     73          11        276   120       Bad  42        17\n2 2 11.22       111     48          16        260    83      Good  65        10\n3 3 10.06       113     35          10        269    80    Medium  59        12\n4 4  7.40       117    100           4        466    97    Medium  55        14\n5 5  4.15       141     64           3        340   128       Bad  38        13\n6 6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n```\n\n\n:::\n:::\n\n\n\n\n### b\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Sales ~ Price + Urban + US, data=cs)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sales ~ Price + Urban + US, data = cs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9206 -1.6220 -0.0564  1.5786  7.0581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.043469   0.651012  20.036  < 2e-16 ***\nPrice       -0.054459   0.005242 -10.389  < 2e-16 ***\nUrbanYes    -0.021916   0.271650  -0.081    0.936    \nUSYes        1.200573   0.259042   4.635 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.472 on 396 degrees of freedom\nMultiple R-squared:  0.2393,\tAdjusted R-squared:  0.2335 \nF-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n### c\n\n- According to the model, 13.04 this is the average sales figures in shops reached in rural areas outside the USA, with the price of child seats still being $0 (not very realis-tic).\n- The coefficient 0.05 indicates that for an increase of one dollar, an average of 0.05 units of child seats are sold less.\n- The coefficient 0.021 means that on average 0.021 less units are sold in urban areas compared to rural areas. However, the p value is very high, so\nthis is more of a random variation.\n- The 1.2 coefficient means that 1.2 more units are sold within the US compared to shops outside the USA. Perhaps child seats are compulsory in the\nUSA.\n\n### d\n\n$$\n\\text{Sales} = \\beta_0 + \\beta_1 \\cdot \\text{Price} + \\beta_2 \\cdot \\text{Urban} + \\beta_3 \\cdot \\text{US}\n$$\n\n> Note: General model!\n\n### e\n\nFor all except `Urban`.\n\n### f\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Sales ~ Price + Urban, data=cs)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sales ~ Price + Urban, data = cs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5324 -1.8441 -0.1443  1.6662  7.5000 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.621458   0.655230  20.789   <2e-16 ***\nPrice       -0.053104   0.005367  -9.895   <2e-16 ***\nUrbanYes     0.034095   0.278293   0.123    0.903    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.535 on 397 degrees of freedom\nMultiple R-squared:  0.198,\tAdjusted R-squared:  0.194 \nF-statistic: 49.01 on 2 and 397 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n### g\n\nThe model `lm(Sales ~ Price + Urban + US, data=cs)` is a mutible regression while the smaller model `model <- lm(Sales ~ Price + Urban, data=cs)` is a simple liniear regression.\n\n# Problems 10\n\n## Problem 10.1\n\n|   | Pop | Lib | NotLib |\n|---|-----|-----|--------|\n| A | 40% | 50% | 50%    |\n| B | 25% | 60% | 40%    |\n| C | 35% | 35% | 65%    |\n\n$$\nP(B|L) = \\dfrac{P(L|B)P(B)}{P(L)}\n$$\n\n- $P(L|B) = 60\\%$\n- $P(B)$ = 25\\%$\n- $P(L) = [P(LA)P(A)]+[P(LB)P(B)]+[P(LC)P(C)] = 0.20+0.15+0.1225=0.4725$\n\n$$\nP(B|L) = \\dfrac{0.6 \\cdot 0.25}{0.4725} \\approx 0.3175 = 31.75\\%\n$$\n\n## Problem 10.2\n\n### a\n\n|         | 1     | 2     | 3     | 4      |\n|---------|-------|-------|-------|--------|\n| Model A | 1/4   | 1/4   | 1/4   | 1/4    |\n| Model B | 1/10  | 2/10  | 3/10  | 4/10   |\n| Model C | 12/25 | 12/50 | 12/75 | 12/100 |\n\n- Model A: Fairest model since all values have the same likelihood.\n- Model B: Unfair model. Value 4 has the highest chance of appearing.\n- Model C: Unfair model. Value 1 has the highest chance of appearing.\n\n### b\n\nFirst try: After 100 throws, the results seem evenly distributed and reflect the distribution from model A.\nSecond try: We can see that value 1 appears much more frequently than value 4. It resembles model C.\n\n## Problem 10.4\n\nSince we don't know whether it's heads or tails, we need to assume a priori a 50% probability for each outcome.\n\n## Problem 10.5\n\n|              | Ice Creame | Fruits | French Fries | Pop |\n|--------------|------------|--------|--------------|-----|\n| 1st graders  | 0.3        | 0.6    | 0.1          | 0.2 |\n| 6th graders  | 0.6        | 0.3    | 0.1          | 0.2 |\n| 11th graders | 0.3        | 0.1    | 0.6          | 0.6 |\n| Overall      | 0.36       | 0.24   | 0.4          | 1   |\n",
    "supporting": [
      "sa_exercises_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}