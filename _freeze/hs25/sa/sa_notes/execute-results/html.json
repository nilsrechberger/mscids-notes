{
  "hash": "1657100e3093ee767234cf2f4327f36c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classical and Bayesian Statistics - Notes\"\ntoc: true\nformat:\n  html: default\n  pdf:\n    include-in-header:\n      - text: |\n          \\newcommand{\\lt}{<}\n          \\newcommand{\\gt}{>}\n---\n\n\n\n\n# Introduction\n\nStatistic is the discipline that concerns collection, organisation, analysis,\ninterpretation, and presentation of data. Applied statistics applies to real everyday problems.\n\n> Note: There is no cooking recipes how to solve problems.\n\n# Classical Statistics\n\nClassical statistics is a set of tools for decision making using hypothesis.\n\n# Bayesien Statistic\nBayesian statistics is a statistical theory that interprets probability as a degree of belief in an event, which can be updated as new evidence is obtained.\n\n> Note: Bayesian statistics is not the same as the Bayesian theorem.\n\n# Models\n\nModels are used to simplify things and are essential for statistics. However, models are only useful in a certain context. Models have their limitations. They are statements about how nature operates that deliberately omit many details, thus achieving insight that would otherwise be obscured.\n\n# Simulations\n\nSimulations are used to approximate quantities for which an exact solution would be very difficult, if not impossible, to determine. They rely heavily on computer power.\n\n# Data\n\n## One-Dimensional\n\nList are the simplest kind od datasets. Lists are heterogeneous data structures.\n\n## Two-Dimentinal\n\nTabels are the most common form of datasets\n\n# Exploratory Datat Analysis (EDA)\n\nThe aim of EDA ist to summarize data by numerical parameters and graphical representation of data. Data should if possible always be graphically displayed and compared with corresponding key figures.\n\n> Note:  Note: whenever a dataset is reduced by key figures or graphics, information gets lost.\n\n# Key Figures\n\n## Location parameters\n\n- Arithemtic mean (average)\n- Median\n- Quantil\n\n## Spread parameters\n\n- Empirical variance\n- Stadnard deviation\n- Interquantile range\n\n## Artithmetic Mean\n\nMean tells a lot about a dataset: “Center” of data. But average does not tell whole story about (quantitative) datasets. Datasets can have a different spread around mean.\n\n$$\n\\bar{x} = \\dfrac{x_1 + x_2 + \\dots{} + x_n}{n} = \\dfrac{1}{n}\\sum^n_{i=1}x_i\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.825\n```\n\n\n:::\n:::\n\n\n\n\n> Note: The arithmetic mean is not robust to outliers.\n\n## Empirical Variancehave\n\nThe value of empirical variance has no physical interpretation.\n\n$$\nVar(x) = \\dfrac{(x_1-\\bar{x})² + (x_2-\\bar{x})² + \\dots + (x_n-\\bar{x})² }{n-1} = \\dfrac{1}{n-1}\\sum^n_{i=1}(x_i - \\bar{x}²)\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nvar(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.3025\n```\n\n\n:::\n:::\n\n\n\n\n## Standard Deviation\n\nStandard deviation is root of variance. Standard deviation has same unit as data itself.\n\n$$\ns_x = \\sqrt{Var(x)} = \\sqrt{\\dfrac{1}{n-1}\\sum^n_{i=1}(x_i - \\bar{x}²)}\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nsd(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.141271\n```\n\n\n:::\n:::\n\n\n\n\n## Median\nAlso called central value or average value. Median is much less influenced by extreme observations than mean.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nmedian(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.7\n```\n\n\n:::\n:::\n\n\n\n\n> Note: Consider Mean and Median simultaneously instead of choosing one.\n\n## Quantiles\nQuantiles are values that divide a dataset into equal parts, allowing for the analysis of the distribution of data. Common types of quantiles include quartiles which split data into four parts.\n\n> Note: Most of the time there is no exact 25 % of observations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nquantile(x) # Default quartil\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   0%   25%   50%   75%  100% \n2.700 3.000 3.700 4.525 5.200 \n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(x, p = 0.7) # Individual value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 70% \n4.39 \n```\n\n\n:::\n:::\n\n\n\n\n# Interquantile Range\n\nMeasure for spread of data.\n\n$$\n\\text{upper quartile} − \\text{lower quartile}\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(4.3, 5.2, 2.7, 3.1)\nIQR(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.525\n```\n\n\n:::\n:::\n\n\n\n\n# Graphical Representation of Data\n\nPlotting data is a very important aspect of statistical data analysis. It often points to patterns that are not recognizable from key figures.\n\n> Note: Choosing the “wrong” graphical representation is not useful.\n\n## Boxplot\n\n![Boxplot: Schematically](img/boxplot.png)\n\n- Box: Height is bounded by the lower and upper quartiles. The height of the box is the interquartile range.\n- Horizontal line in box: Median\n- Whisker: $1.5 \\times \\text{IQR}$, defined by inventor John Tukey.\n- Points: Outliers\n\n> Note: The upper and lower whiskers do not have to be $1.5 \\times \\text{IQR}$ in length.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Single boxplot\nset.seed(42) # Set seed for reproducibility\nx <- rnorm(10) # Generate 10 random samples\n\nboxplot(x) # Plot data\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare two sample groups\nset.seed(42)\nx <- rnorm(100)\ny <- rnorm(100)\n\nboxplot(x, y)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n## Histogram\n\nGraphical overview of occurring values. Draw a bar for each class, with the height proportional to the number of observations in that class.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple histogram\nset.seed(42)\nx <- rnorm(1000)\n\nhist(x)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nThe selection of the number of classes is relevant for the interpretation of a histogram.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Big breaks\nset.seed(42)\nx <- rnorm(1000)\n\nhist(x, breaks=5)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Small breaks\nset.seed(42)\nx <- rnorm(1000)\n\nhist(x, breaks=50)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n> Note: Since we used `set.seed(42)`, both plots show the same data.\n\n### Skweness of Hostograms\n\nHistograms can have a skewness depending on the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Right skewed data\nset.seed(42)\nx <- rexp(1000, rate = 1) # Using expontial distribution\n\nhist(x, breaks=50)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Left left data\nset.seed(42)\nx <- rexp(1000, rate = 1)\nx <- -x # Trun data positivity\n\nhist(x, breaks=50)\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n> Note: The terms “right” and “left” always refer to the direction where there is less data (the tail of the distribution).\n\n## Normalised Histograms\n\nSelect the bar height such that the bar area corresponds to the proportion of respective observations in the total number of observations.\n\n> Note: Density values are not percentages.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normalized histogram\nset.seed(42)\nx <- rnorm(1000)\n\nhist(x, freq=FALSE) # Disable frequency\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n## Boxplot X Histogram\n\n![Boxplot X Histogram](img/boxplot_x_histogram.png)\n\n# Probability\n\n- A random element: $x = 2$\n- Set of elements: $X = {1, 2, 3, 4}$\n- $x$ is an element of the set $X$: $x \\in X$\n- Subset: If $B = {2, 3}$, then $B \\subset A$\n- Empty set: $\\emptyset$. The empty set is a subset of every set.\n\n## Probability Model\n\nProbability models use set theory as a language. For random experiments, the outcome is not predictable. A probability model consists of events that are possible in such an experiment and probabilities for different results occurring. Probability models have the following components:\n\n- Sample space $\\Omega$: Contains all possible elementary events $\\omega$.\n- Events $A, B, C$: Subsets of the sample space.\n- Probabilities $p$ associated with events $A, B, C$.\n- Event: More general and more important than elementary events, but consists of them.\n\nProbability formula:\n\n$$\np(E)=\\dfrac{\\lvert E \\rvert}{\\lvert \\Omega \\rvert} \\text{, where } E \\subseteq \\Omega\n$$\n\n## Set Theory\n\n- Union: $A \\cup B$\n- Intersection: $A \\cap B$\n- Complement: $\\bar{A}$\n- Difference: $A \\setminus B$\n\n![Set theory](img/set_theory.png)\n\n## Axioms of Probability\n\nEach event $A$ probability $P(A)$ is assigned, with properties:\n\n- $p(A) \\geq 0$\n- $p(\\Omega) = 1$\n- $p(A \\cup B) = p(A) + p(B) \\text{, if } A \\cap B = \\emptyset$\n\n> Note: Kolmogorov Axioms of Probability\n\n## Laws for Calculating Probabilities\n\nIf $A, B$ and $A_1, \\dots, A_n$ events, then:\n\n- $p(\\bar{A}) = 1 - p(A) \\text{, for all } A$\n- $p(A \\cup B) = p(A) + p(B) - p(A \\cap B) \\text{, for all } A \\text{ and } B$\n- $p(A_1 \\cup \\dots A_n) \\leq p(A_1) + \\dots + p(A_n) \\text{, for all } A_1, \\dots, A_n$\n- $p(B) \\leq p(A) \\text{, for all } A \\text{ and } B \\text{ with } B \\subseteq A$\n- $p(A \\setminus B) = p(A) - p(B) \\text{, for all } A \\text{ and } B \\text{ with } B \\subseteq A$\n\n## Discrete Probability Models\n\nA sample space can be finite or infinite and discrete. It can also be infinite but still discrete.\n\n### Probabilities for Discrete Models\n\nThe probability of event $A = {\\omega_1, \\omega_2, \\dots, \\omega_n}$ is determined by the sum of the probabilities $p(\\omega)$ of the corresponding elementary events.\n\n$$\np(A) = \\sum_{\\omega_i \\in A} p(\\omega_i)\n$$\n\n\n## Laplace Model\n\nProbabilities of all elementary elements add up to 1.\n\n$$\np(E) = \\dfrac{f}{p} = \\sum_{k:\\omega_i \\in E} p(\\omega_k)\n$$\n\nDivides number of “favorable” elementary events by number of “possible” elementary events.\n\n## Stochastic Independence\n\nIf events $A$ and $B$ are stochastically independent, then:\n\n$$\nP(A \\cap B) = P(A) \\cdot P(B)\n$$\n\n> Note: Formula applies only if events A and B are stochastically independent.\n\nOutcome of event A has no influence on outcome of event B and vice versa.\n\n# Random Variable & Probability Distribution\n\n## Random Variable\n\nA Random variable X is a function:\n\n$$\n\\Omega \\to W_X \\subset \\mathbb{R}\n$$\n$$\n\\omega \\to X(\\omega)\n$$\n\nRandom variable denoted by capital letters X (or Y, Z). Corresponding lowercase letters x (or y, z) represents specific value that random variable can take. Once $\\omega$ is chosen: $X(\\omega)$ is fixed, not random.\n\n> Note: $x$ also called a realisation of random variable $X$.\n\n## Probability Distribution of Random Variable\n\nValues of random variable X (possible realisations of X) occur with certain probabilities that $X$ takes value $x$:\n\n$$\nP(X = x) = P(\\{\\omega \\mid X(\\omega) = x\\}) = \\sum_{\\omega; X(\\omega)=x} P(\\omega)\n$$\n\n\n## Probability Distribution\n\nAssociated probability is determined for all realisations of random variable. List of $P(X = x)$ for all possible values $x_1, x_2, \\dots, x_n$ is called discrete probability distribution of discrete random variable $X$.All values of probability distribution has to sum up to 1.\n\n$$\n\\sum_{\\text{For all }x} P(X = x) = 1\n$$\n\n> Note: For finite sample space the probability distribution is a table.\n\n## Key Figures of Distribution\n\n### Expected Value $E(X)$\n\nCentral location of distribution. Weighted mean of all possible values, weighted by their probability of occurring.\n\n$$\n\\mu = \\sum_{\\text{all possible } x} x \\times P(X = x)\n$$\n\n> Note: $E(X)$ is a theoretical value, which results from a model, i.e. distribution.\n\n### Variance\n\nVariance is square of spread of value of random variable from expected value weighted with respective weight.\n\n$$\n\\sigma = \\sum_{\\text{all possible } x} (x - E(X))^2 \\times P(X = x)\n$$\n\n### Standard deviation $\\sigma$\n\nSpread of distribution about $E(X)$. Standard deviation has same unit as X.\n\n$$\n\\sigma(X) = \\sqrt{Var(X)}\n$$\n\n> Note: $\\sigma(x)$ is a theoretical value, which results from a model, i.e. distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1 : 6\np <- 1 / 6\nE_X <- sum(x * p)\nvar_X <- sum((x - E_X)^2 * p)\nsd_X <- sqrt(var_X)\n\nsd_X\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.707825\n```\n\n\n:::\n:::\n\n\n\n\n> Note: Means: Deviation on “average” 1.7 from 3.5.\n\n# Week 04: Continuous Distributions & Normal Distribution\n\n## Continuous Probability Distribution\n\nFor continuous probability distributions, probabilities correspond to areas under density function. Range $W_X$ of a random variable: Set of all values $X$ can take. Random variable $X$ is continuous, if its range $W_X$ is continuous. For continuous random variable $X$ for all $x \\in W_X$:\n\n$$\nP(X = x) = 0\n$$\n\n> Note: Probability distribution of $X$ can not be described by $P(X = x)$.\n\n## Probability Density\n\nProbability density function f(x) has the properties:\n\n- Function is not negative: $f(x) \\ge 0$\n- Probability corresponds to area between $a$ and $b$ under $f(x)= P(a \\lt X \\le b )$\n- Total area under curve is $1$\n\n> Note: Values of f(x) are not probabilities, only areas are.\n\n## Quantiles of Continuous Probability Distribution\n\nFor continuous distributions, the $\\alpha$ quantile $q_\\alpha$ is value where area (probability) under density function from $-\\infty$ to $q_\\alpha$ is just $\\alpha$.\n\n![Quantiles of Continuous Probability Distribution](img/contProbDistrAlpha.png)\n\n## Normal Distribution\n\nThe normal distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ have the desity function:\n\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right\\}\n$$\n\nWith the expected value $E(X) = \\mu$ and variance $Var(X) = \\sigma^2$. The parameter $\\mu$ shifts the curve horizontally from origin while $\\sigma$ defines the shape of the curve.\n\n### Properties\n\n- Approx. $\\frac{2}{3}$ of are between $\\mu \\pm \\sigma$\n- Approx. 95% of area between $\\mu \\pm 2\\sigma$\n\n### Calculate Normal Distributed Probability\n\nCalculation of IQ $P(X \\le 130)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mean and sd are predefined in this task\npnorm(q = 130, mean = 100, sd = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9772499\n```\n\n\n:::\n:::\n\n\n\n\nsince total area under curve is 1, $P(X \\gt 130)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(q = 130, mean = 100, sd = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02275013\n```\n\n\n:::\n:::\n\n\n\n\nDetermine quantiles:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(p = 0.025, mean = 100, sd = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 70.60054\n```\n\n\n:::\n\n```{.r .cell-code}\nqnorm(p = 0.975, mean = 100, sd = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 129.3995\n```\n\n\n:::\n:::\n\n\n\n\n## i.i.d Assumption\n\nThe i.i.d. assumption means a set of random variables are Independent (the value of one doesn't affect the others) and Identically Distributed (all come from the exact same probability distribution).\n\n### Key Figures\n\n#### $S_n$\n\n- $E(S_n) = n\\mu$\n- $\\text{Var}(S_n) = n\\text{Var}(X_i)$\n- $\\sigma(S_n) = \\sqrt{n}\\sigma_X$\n\n#### $\\bar{X}_n$\n\n- $E(\\bar{X}_n) = \\mu$\n- $\\text{Var}(\\bar{X}_n) = \\frac{\\sigma^2_X}{n}$\n- $\\sigma(\\bar{X}_n) = \\frac{\\sigma_X}{\\sqrt{n}}$\n\n> Note: Standard deviation of $X_n$ is called standard error of arithmetic mean.\n\n\n## Central Limit Theorem\n\nThe Central Limit Theorem (CLT) states that, given a sufficiently large sample size ($n$), the distribution of the sample mean ($\\bar{X}_n$) will be approximately normally distributed, regardless of the original population's distribution. This is true as long as the population has a finite mean ($\\mu$) and variance ($\\sigma^2$). In short: For large samples, the sample mean is normally distributed.\n\n# Week 05: Hypothesis Tests\n\nHypothesis testing are a important statistical tool to decide whether observations “fits” a certain parameter. Ro introduce a standardised, reproducible procedure to decide\nwhether mean of observations does (or not) match a certain true mean $\\mu$.\n\n> Note: We can only show that this quantity does not fit to the observations with high probability.\n\n## Estimation\n\nPoint estimates for expected value.\n\n$$\n\\hat{\\mu} = \\dfrac{1}{n}\\sum^n_{i=1}X_i\n$$\n\nPoint estimates for variance.\n\n$$\n\\hat{\\sigma}^2_X = \\dfrac{1}{n-1}\\sum^n_{i=1}(X_i - \\bar{X_n})^2\n$$\n\n> Note: Hat $\\hat{x}$ denotes estimate of a quantity.\n\n## Procedure Hypothesis Test\n\nUsing a observation, check whether, under assumption $\\mu = x$, mean of observations is probable or not.\n\n- $\\mu$: True (unknown) mean of data\n- $\\mu_0$: Assumed true mean of data\n\nNull Hypothesis\n\n$$\nH_0: \\mu = \\mu_0 = x\n$$\n\nAlternative Hypothesis\n\n$$\nH_A: \\mu \\ne \\mu_0 = x\n$$\n\nTest with this distribution whether assumption μ = x is justified. Distribution of test statistic $T$ under the null hypothesis $H_0$\n\n$$\nT: \\bar{X}_{10} \\sim \\mathcal{N}(x, \\dfrac{1^2}{x})\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assume real mean of 500\npnorm(q = 499.22, mean = 500, sd = 1/sqrt(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006820578\n```\n\n\n:::\n:::\n\n\n\n\nIt has proven practical to set this limit of what is too small and what is not at 2.5 %. So assume that given mean of $μ_0 = 500$ is not plausible. We reject null hypothesis.\n\n## Significance Level \\alpha\n\nSignificance level $\\alpha$, indicates how high a risk one is willing to take of making a wrong decision. For most tests $\\alpha$ value of $0.05$ or $0.01$. Boundary rejection range $0.025$- and $0.975$-quantiles.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(p = c(0.025, 0.975), mean = 500, sd = 1/sqrt(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 499.3802 500.6198\n```\n\n\n:::\n:::\n\n\n\n\nIf observed mean lies in red area of Figure, null hypothesis is rejected, also called rejection range.\n\n## p-Value\n\np-value is probability of observing an event under null hypothesis that is at least as extreme (in direction of alternative) as currently observed event. The smaller p-value, the more result argues against null hypothesis. Values smaller than a predetermined limit, such as 5 %, 1 % or 0.1 % are reason to reject the null hypothesis.\n\n# Week 06: t-Test, Wilcoxon-Test, Confidence Interval\n\n### $t$-Distribution\n\nDistribution of test statistics for t-test under null hypothesis. Similar to normal distribution, but flatter, due to greater uncertainty. Depends on number of observations. Symmetric distribution around $0$, but flattens out slower than standard normal distribution $\\mathcal{N} (0, 1)$\n\n![t-Distribution](img/t_distr.png)\n\n$$\nH_0 = \\mu =\\mu_o\n$$\n\nis given by\n\n$$\nT = \\bar{X}_n \\sim t_{n-1} \\left(\\mu, \\frac{\\hat{\\sigma}_{\\bar{X}}^2}{n}\\right)\n$$\n\nwhere $t_{n−1}$ is a $t$-distribution with $n − 1$ degrees of freedom.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(5.9, 3.4, 6.6, 6.3, 4.2, 2.0, 6.0, 4.8, 4.2, 2.1, 8.7, 4.4, 5.1, 2.7, 8.5, 5.8, 4.9, 5.3, 5.5, 7.9)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.215\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.883802\n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(x, mu = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  x\nt = 0.51041, df = 19, p-value = 0.6156\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.333353 6.096647\nsample estimates:\nmean of x \n    5.215 \n```\n\n\n:::\n:::\n\n\n\n\n## Confidence Interval\n\nInterval indicating where, roughly speaking, true mean lies with a certain predefined probability\n\n### Test Decision with Confidence Interval\n\nIf $\\mu_0$ of null hypothesis does not lie within confidence interval of $\\bar{X}_n$, $H_0$ is rejected.\n\n## Non-Normally Distributed Data: Wilcoxon Test\n\nAssumes less than t-test. Distribution under null hypothesis is symmetrical with respect to median $\\mu_0$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(79.98, 80.04, 80.02, 80.04, 80.03, 80.03, 80.04, 79.97, 80.05, 80.03, 80.02, 80.00, 80.02)\n\nwilcox.test(x, mu = 80.00, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x, mu = 80, alternative = \"two.sided\"): cannot\ncompute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x, mu = 80, alternative = \"two.sided\"): cannot\ncompute exact p-value with zeroes\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  x\nV = 69, p-value = 0.0195\nalternative hypothesis: true location is not equal to 80\n```\n\n\n:::\n:::\n\n\n\n\n### Wilcoxon test vs. t-test\n\nWilcoxon test is in the vast majority of cases preferable to the t-test: It often has much greater power in many situations (probability of correctly rejecting the null hypothesis). Even in the most extreme cases it is never much worse.\n\n## Paired Samples\n\nBoth observations are not independent, because same experimental unit is measured twice. Each observation of one group can be clearly assigned to an observation of the other group. Sample size is inevitably same in both groups\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbefore <- c(25, 25, 27, 44, 30, 67, 53, 53, 52, 60, 28)\nafter <- c(27, 29, 37, 56, 46, 82, 57, 80, 61, 59, 43)\n\nt.test(x = before, y = after, paired=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  before and after\nt = -4.2716, df = 10, p-value = 0.001633\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -15.63114  -4.91431\nsample estimates:\nmean difference \n      -10.27273 \n```\n\n\n:::\n:::\n\n\n\n\n## Unpaired (Independent) Samples\n\nSo-called unpaired (or independent) samples. No assignment of observations possible. Sample sizes can be differen.\n\n### Mann-Whitney U-Test (aka Wilcoxon Rank-sum Test)\n\nIf data are non-normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(79.98, 80.04, 80.02, 80.04, 80.03, 80.03, 80.04,79.97, 80.05, 80.03, 80.02, 80.00, 80.02)\ny <- c(80.02, 79.94, 79.98, 79.97, 80.03, 79.95, 79.97)\n\nwilcox.test(x, y, alternative=\"two.sided\", mu=0, paired=FALSE, conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(x, y, alternative = \"two.sided\", mu = 0, :\ncannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  x and y\nW = 76.5, p-value = 0.01454\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n## Interpreting p-Values\n\nDetermine whether hypothesis test results are statistically significant. If p-value is less than significance level, reject null hypothesis and conclude that effect or relationship exists. The p-values are directly connected to null hypothesis.\n\n::: {.callout-note}\np-values: Probability of observing a sample statistic that is at least as extreme as sample statistic when assuming that null hypothesis is correct\n:::\n\n> Note: p-Values Are **not** an Error Rate.\n\n### What Is True Error Rate?\n\nCan’t directly calculate error rate based on a p-value.\n\n# Week 07: Linear Regression\n\n## Scatter plot\n\nTwo observations interpreted and displayed as coordinates of points in a coordinate system.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Generate random data\nx <- rnorm(100)\ny <- rnorm(100)\n\nplot(x, y) # By defaults Scatter Plot\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n## Dependence and Causality\n\nCaution regarding scatter plots: Do not confuse dependence with causality. Caution regarding scatter plots: Do not confuse dependence with causality.\n\n## Equation of a Line\n\n$$\ny = a+bx\n$$\n\n$$\nb=\\dfrac{\\Delta y}{\\Delta x}\n$$\n\n## Linear Regression\n\nFormula-based relationship between book $x$ and $y$. The Problem is to find a line that fits all points as good as possible.\n\n### Residuals\n\nDistance between points and line.\n\n::: {.callout-note}\n\nA residual $r_i$ is the vertical difference between a data point ($x_i$ , $y_i$ ) and the point ($x_i$ , $a + bx_i$ ) on the sought line:\n\n$$\nr_i = y_i - (a + bx_i) = y_i - a - bx_i\n$$\n\n:::\n\n### Least Squares Method\n\nDetermine a and b so that the sum of the squared residuals becomes minimal.\n\n$$\nr_1^2 + r_2^2 * \\dots + r_n^2 = \\sum_i r_i^2\n$$\n\n### Parameter a, b minimise (least squares method)\n\n$$\n\\sum_{i=1}^n (y_i - (a+bx_i))^2\n$$\n\n$$\nb = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n$$\n\n$$\na = \\bar{y} - b\\bar{x}\n$$\n\n> Note: $\\bar{x}$ and $\\bar{y}$ are the mean values of the respective data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Generate random linear data\nx <- rnorm(100)\ny <- x * rbinom(100, size=100, prob=0.5)\n\nplot(x, y)\nabline(lm(y ~ x), col=\"red\") # Regression line\n```\n\n::: {.cell-output-display}\n![](sa_notes_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n> Note: It is `lm(y~x)` and not `lm(x~y)`.\n\n## Empirical correlation\n\nThe empirical correlation is a dimensionless number between −1 and +1 and measures strength and direction of the linear dependency between the dates $x$ and $y$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nx <- rnorm(100)\ny <- x * rbinom(100, size=100, prob=0.5)\n\ncor(x, y) # High value because of pos linear data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9953896\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-warning}\nEmpirical correlation only measures the linear correlation.\n:::\n\n## $R^2$ Value\n\n$R^2$-statistics: Value between 0 and 1. It indicates to what proportion of the variability in $Y$ is explained by $X$ using the model. Value close to 1: A large proportion of the variability is explained by the regression. The model therefore describes the data very well.\n\n> Note: $R^2$ can be used for any regression.\n\n```r\nsummary(lm(Sales~TV))$r.squared\n```\n\n## F-Statistic\n\nThe F-statistic is a value resulting from an F-test, which evaluates the overall significance of a regression model. Unlike the t-test, which looks at individual predictors, the F-test looks at the model as a whole.\n\n- High F-value: Indicates that the model explains a significant amount of the variation in the data.\n\n- Low F-value (near 1): Suggests that the observed patterns could likely be due to random chance.\n\n# Week 08: Multiple Linear Regression\n\nSimple linear regression are useful procedure to predict output based on one single predictor. In practice the output often depends on more than one predictor. Multiple linear model generalises simple linear model. Calculations and interpretations for multiple model similar, although usually more complicated than linear model\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon\n$$\n\n```r\n# Same syntax as in linear models\nmodel <- lm(Sales ~ TV + Radio + Newspaper)\n```\n\n## Interpretation of Coefficients\n\nThe interpretation of the coefficients is similar to the interpretation in a linear regression model.\n\n- $\\beta_0$​ (Intercept): $\\beta_0$​ represents the estimated mean value of the dependent variable $Y$ when all predictor variables $(X_1​, X:2​, \\dots, X_p​)$ are equal to zero.\n- $\\beta_i$​ (Partial Effect): A coefficient $\\beta_i$ represents the estimated change in the mean of $Y$ for a one-unit increase in the predictor variable $X_i​$, while holding all other predictor variables constant.\n\n### Estimation of Regression Coefficients\n\nThe regression coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ generally unknown. We estimate those based on the data by using a a cost function like RSS.\n\n$$\nRSS = \\sum_{i=1}^n r_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_p x_{ip})^2\n$$\n\n```r\ncoef(lm(Sales ~ TV + Radio + Newspaper))\n\n## (Intercept) TV          Radio       Newspaper\n## 2.938889369 0.045764645 0.188530017 -0.001037493\n```\n\n::: {.callout-important}\nSlope for *Newspaper* describes change in response *Sales* when spending *CHF 1000* more on newspaper advertising, **while other two predictors *TV* and *radio* are hold constant**\n:::\n\n### Correlation coefficients\n\n```r\ncor(data.frame(TV, Radio, Newspaper, Sales))\n\n##           TV         Radio      Newspaper  Sales\n## TV        1.00000000 0.05480866 0.05664787 0.7822244\n## Radio     0.05480866 1.00000000 0.35410375 0.5762226\n## Newspaper 0.05664787 0.35410375 1.00000000 0.2282990\n## Sales     0.78222442 0.57622257 0.22829903 1.0000000\n```\n\n## Relationship between Predictors and Response Variable\n\nMultiple linear regression with $p$ predictors: All regression coefficients except $\\beta_0$ are zero, no variable has influence (Null hypothesis). As a alternative hypothesis: At least one $\\beta_i$ is not equal to $0$.\n\n## No Linear Regression\n\nThe moderated effect, often referred to as an interaction effect, summarizes a statistical relationship where the influence of one variable is not constant but instead depends on the value of a second variable.\n\n```r\nmodel -> lm(Sales ~ TV + Radio + TV * Radio)\n```\n\n> Note: The asterisk $*$ in R is an abbreviation for the addition of the main effects AND the interaction term.\n\n# Week 09: Conditional Probability\n\nConditional probability is probability that event A occurs when\none already knows that B has occurred\n\nGeneral formula:\n$$\nP(A \\cap B) = \\dfrac{A \\cap B}{\\Omega}\n$$\n\nFormula is used to define conditional probability:\n$$\nP(A | B) = \\dfrac{P(A \\cap B)}{P(S)}\n$$\n\n::: {.callout-caution}\n$P(A|B) \\ne P(B|A)$\n:::\n\n## Bayes Theorem\n\n$$\nP(A | B) = \\dfrac{P(B | A) P(A)}{P(B)}\n$$\n\n## Law of Total Probability\n\nf $A_1, \\dots, A_k$ is a partition of A and B an event, then:\n\n$$\n\\sum_{i=1}^k P(B | A_k) P(A_k)\n$$\n\n# Week 10: Bayesian Statistics Introduction\n\n## Bayesian Statistics\n\nBayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability, where probability expresses a degree of belief in an event.\n\nIn general:\n\n$$\nP(r \\cap c) = P(r | c)P(c)\n$$\n\nBut also:\n\n$$\nP(r \\cap c) = P(c | r)P(r)\n$$\n\nTherefore:\n$$\nP(c | r)P(r) = P(r | c)P(c)\n$$\n\nHence:\n$$\nP(c | r) = \\dfrac{P(r | c)P(c)}{P(r)}\n$$\n\n## Bayes’ Theorem: Prior, Likelihood, Posterior\n\nIntroduce the following terms:\n\n- $P(M)$: Prior probability\n- $P(M | +)$: Posterior probability\n- $P(+ | M)$: Likelihood function\n\n# Week 11: Beta distribution\n\n## Evidence `P(D)`\n\nCalculate evidence P(D) with law total probability.\n\n$$\n\\sum_{i=1}^n P(D|\\theta_i)P(\\theta_i)\n$$\n\nUse probability density functions instead of probabilities and sums become integrals.\n\n$$\nP(D) = \\int P(D \\mid \\theta^*) p(\\theta^*) d\\theta^*\n$$\n\n> Note: Areas under probability density curves correspond to probabilities. Total ares under the curve is 1.\n\n## Bernoulli distribution\n\n### Likelihood function for coin tosses\n\nIf N denotes number of tosses, $z$ number of $H$ and $N − z$ number\nof $T$, then probability distribution:\n\n$$\nP(\\{y_i\\} \\mid \\theta) = \\theta^{Z}(1 - \\theta)^{N - Z}\n$$\n\n## Description of Probabilities: Beta Distribution\n\nThe Beta distribution ($Beta(α,β)$) is a continuous probability distribution defined over the interval $[0,1]$. The Beta distribution is controlled by two positive shape parameters, typically denoted as $α$ and $β$. By changing $α$ and $β$, the distribution can take on a wide range of shapes, including symmetric, skewed left or right, or even U-shaped.\n\nThe ratio of $\\alpha$ to $\\beta$ determines where the peak (the mode) lies on the x-axis.\n\n* **$a > b$:** There are more successes than failures. The probability $\\theta$ is likely high.\n    * **Effect:** The peak shifts to the **right** (above 0.5).\n* **$b > a$:** There are more failures than successes. The probability $\\theta$ is likely low.\n    * **Effect:** The peak shifts to the **left** (below 0.5).\n* **$a = b$:** Successes and failures balance each other out.\n    * **Effect:** The peak is exactly in the **middle** (at 0.5).\n\nThe sum $\\alpha+\\beta$ (often called $n$ or concentration) determines how narrow or wide the curve is. This is the \"weight\" of your experience.\n\n* **$a$ and $b$ are small (e.g., 2 and 2):**\n    * You have hardly seen any data. Although it's 50/50, you are uncertain.\n    * **Shape:** A flat, wide bump. The HDI is huge.\n* **$a$ and $b$ are large (e.g., 100 and 100):**\n    * You have seen a lot of data. You are very sure that it is 50/50.\n    * **Shape:** A tall, sharp needle. The HDI is tiny.\n\n### Central Tendency\n\nOur goal is to transform a prior belief, which is expressed in terms of tendency and uncertainty, into corresponding parameter values $a$ and $b$ in the beta distribution.\n\n### Mean\n\n$$\n\\mu = \\dfrac{a}{a+b}\n$$\n\n### Mode\n\n$$\n\\omega = \\dfrac{a-1}{a+b-2}\n$$\n\n### Spread\n\n$$\nk = a + b\n$$\n\nFor $a$ and $b$ in terms of mean $μ$ and concentration $κ$\n\n$$\na = \\muκ\n$$\n\n$$\nb = (1-\\mu)κ\n$$\n\nFor $a$ and $b$ in terms of mode $ω$ and concentration $κ$\n\n$$\na = \\omega(κ − 2) + 1 \n$$\n\n$$\nb = (1 − \\omega)(κ − 2) + 1 \\text{ for } κ > 2\n$$\n\n## Posterior Beta\n\nIf prior distribution is beta distribution Beta(θ | a, b), and data show z heads in N tosses, then posterior distribution is again beta distribution.\n\n$$\np(\\theta \\mid z, N) = \\text{Beta}(\\theta \\mid z + a, N - z + b)\n$$\n\n## Highest Density Interval (HDI)\n\nTo summarise a distribution, use highest density interval (HDI). It indicates which points of a distribution are most credible and\nwhich represent largest part of the distributio.\n\n- Wide HDI (large distance between start and end): There are many different values that could be plausible. The probability is distributed “widely” across the x-axis. We cannot narrow down the true value very well.\n\n- Narrow HDI (small distance): The probability is concentrated in a very small range. We are very certain where the true value lies.\n\n::: {.callout-note}\nSince this normal distribution is symmetric about zero, the 95 %-HDI extends from −1.96 to +1.96.\n:::\n\n> Note: HDI has nothing to do with confidence interval.\n\n## ROPE\n\nThe ROPE is the “range of tolerance” that you define before the experiment. It prevents us from misinterpreting tiny, insignificant deviations as “significant” just because we have a lot of data.\n\n| Relationship between HDI and ROPE | Interpretation | Decision |\n| :--- | :--- | :--- |\n| **HDI is completely inside the ROPE** | The entire range of credible values is practically equivalent to the null value. | **Accept** the null value (practical equivalence). |\n| **HDI is completely outside the ROPE** | The entire range of credible values is different from the null value. | **Reject** the null value. |\n| **HDI and ROPE overlap** | Some credible values are practically equivalent, others are not. | **Undecided / Inconclusive** (more data needed). |\n\n## Influence of prior on posterior distribution\n\nPractical relevance: It does not matter whether choice $a = b = 10$ or $a = b = 15$ for prior distribution. It does matter whether $a$ = b = 1$ or $a = b = 10$ or $a = b = 100$. If we have no idea about coin: Choose $a = b = 1$. If we have examined the coin more closely and find that it is very symmetrical, we may choose $a = b = 100$.\n\n# Week 12: General Metropolis Algorithm\n\n## Markov Chain Monte Carlo (MCMC)\n\nPresent methods to generate good approximations of Bayes’ posterior distributions. Method described starts from two assumptions:\n\n- Prior distribution: Function that can be easily evaluated by a computer.\n- Likelihood function: Function that can be easily evaluated by a computer.\n\nPosterior distribution is estimated by randomly generating a set of $\\theta$ values from it\n\n## Approximation of a Distribution Mean Large Samples\n\nConcept of representing a distribution using a large sample of representative $\\theta$ values is fundamental to approach to Bayesian analysis of complex models. The larger the sample, the better the estimate.\n\n## Metropolis Algorithm\n\nApproximation of continuous posterior distribution. Metropolis algorithm generates many representative $\\theta$ values whose histogram approximates posterior distribution given a sufficiently large number of sample $\\theta$ values.\n\n## Region of Practical Equivalence (ROPE)\n\nSpecifies a small range of parameter values that is considered practically equivalent to null value for purposes of a particular application.",
    "supporting": [
      "sa_notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}