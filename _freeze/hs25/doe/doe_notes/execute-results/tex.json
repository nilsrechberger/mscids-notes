{
  "hash": "3d859955bcb0646eaad30bb58f84f8b7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Design of Experiments\n---\n\n\n\n\n\n\n# Research Design\n\n## Observational vs. Experimental\nThe fundamental difference between observational studies and experimental research designs is that in the former, researchers simply observe and measure variables without actively intervening. In the latter, variables are purposefully manipulated to determine a cause-and-effect relationship.\n\n## Randomization\nThe effect and aim of randomisation is to eliminate selection bias and confounding factors, and to ensure comparability between groups at the start of the study (baseline data).\n\n## Confounder\nConfounding occurs when a factor (confounder) that has not been investigated is associated with both the independent and dependent variables, causing a spurious correlation between them.\n\n## Blinding\nThe problem is distortion due to knowledge about the treatment. Blinding is a suitable technique for avoiding such distortions. It eliminates conscious and unconscious influences on the treatment result.\n\n### Types of blining\n\n- Open: No blining\n- Single-blind: Participants don't know their group assignment (e.g., whether they're receiving the real treatment or a placebo).\n- Double-blind: Neither the participants nor the researchers administering the treatment know the group assignments.\n- Triple-blind: Participants, researchers, and the data analysts are all unaware of the group assignments.\n\n# Principles\n\n##  Introduction to scientific theory\n\n### Scientific Theory\n\nExamines whether and how scientific knowledge can be obtained. It is a branch of philosophy that deals with the theory of scientific knowledge and scientific methods, as well as with research. It also analyzes the practices that generate scientific knowledge and examines the institutional and social contexts in which these take place.\n\n### Methodology\n\nIt focuses on the underlying considerations, decisions, and justifications of the approach used in scientific research projects. It provides the instructional framework on how to proceed in order to gain scientific knowledge. It does not comprise a strictly formal set of rules but offers a diverse and pragmatic set of choices that are linked to human action.\n\n### Research Methodes\n\nSystematized procedures and approaches for obtaining knowledge.\n\n### Selection of positions held in scientific theory\n\n- Classical rationalism: Reason precedes experience and there are so-called \"innate\" concepts of reason.\n- Inductive empiricism: Findings are derived inductively based on observations and experiences.\n- Logical positivism: The use of logic makes it possible to separate science from metaphysics.\n- Critical rationalism: Findings are derived deductively based on observations.\n- (Social) constructivism: Individuals construct their reality by relating their thinking and actions.\n\n> Note: Inductive empiricism and critical rationalism belong to empirical research.\n\n### What is empirical research?\n\nKnowledge can be gained only through observation, experiment, and experience. Empirical research examines the environment by means of observation and experiment. There are many research methods for conducting observations and experiments:\n\n- Interview\n- Case study\n- Survey study\n- Experiment\n\n### Landscape of empirical research\n\n![Landscape of empirical research](img/landscape_of_empirical_research.png)\n\n### Quantitative Methodes\n\nResearch that uses quantitative methods is designed around the principles of critical rationalism. The approach assumes that a theory can never be finally verified, it can only be falsified.\n\n### Descriptive statistics\n\n- Describes the data to be analyzed.\n- Is limited to a sample as a subset of the population.\n- Does not allow for conclusions to be drawn about the population.\n\n### Inferential statistics\n\n- For drawing conclusions about the population based on information obtained from a sample.\n- Use statistical hypothesis tests, especially, as the main component.\n\n### Hypothesis Testing\n\nHypothesis testing is a statistical method used to determine if there is enough evidence in a sample of data to support a specific hypothesis about a population. It involves formulating a null hypothesis and an alternative hypothesis, analyzing sample data, and making a decision based on the results, often using a p-value to assess significance.\n\n- Alternative hypothesis ($H_A$): Research hypothesis to be tested that postulates the presence of a certain effect (e.g. a difference) in the population.\n- Null hypothesis ($H_0$): Postulates the opposite, namely the absence of an effect.\n\n## Research process\n\n### Phases\n\n1. Formulation of the research problem & study design\n2. Planning and preparation of the study\n3. Data collection\n4. Data Analysis\n5. Reporting\n\n### Measuring Instrument\n\nA process that uses a given set of circumstances to define and specify subsequent research steps with a view to better understanding these circumstances.\n\n### Sampling procedure\n\nA selection of cases derived from the population and compiled for research purposes results in statements as part of an empirical study. Sampling often involves people, but objects of all kinds (e.g., websites, newspaper articles, companies, countries) can also form a population. Sample surveys are typical of empirical social research. Only rarely are censuses used that examine all cases associated with the population.\n\n## Definition & properties of study design\n\nThe choice of a suitable research design determines the scientific quality of a study. The planning of the analysis depends on the research design.\n\n### Study types\n\n- Descriptive study: Descriptive character. Suitable for forming hypotheses (Surveys).\n- Analytical study: Identification and quantification of effects / verification of relationships. Not fully suitable for hypothesis testing (Cohot).\n- Randomized controlled: Suitable for hypothesis testing (RCT).\n\n# Lecture 03: Introduction to Design of Experiments (DoE)\n\n## Cause and Effect\n\nA trial / experiment is carried out to discover a cause-and-effect relationship in a process.\n\n### Terms\n\n- Input: Trial objects, test objects, test persons, ect.\n- Process: Process in which controllable and non-controllable factors influence the input.\n- Output (aka. Dependet variable: DV): Input changed by the process, result of the test/experiment.\n- Controllable factors (aka Independet variables: IV): Influencing factors whose strength can be adjusted within defined limits.\n- Non-controllable factors: Influencing factors whose strength cannot be determined but that can be measured / cannot be determined and that cannot be measured.\n\n## Non-controllable factors\n\nNon-controllable factors are also referred to as nuisance variables in a general context or nuisance factors in the context of blocking\n\n> Note: Blocking = arranging of experimental units in groups (blocks)\n\n## Causality in observational and experimental study designs\n\nObservational studies cannot directly prove causality, but only show correlations or associations. Since the assignment is not random, there is always a risk that the results are distorted by unknown confounding factors.\n\nExperimental studies (e.g. RCTs) can prove causality because they control for confounding factors through randomisation, thereby isolating the effect of the cause. They are the gold standard.\n\n## Variance\n\nThe variance describes the mean square deviation of the individual measured values from the empirical mean.\n\n- Primary variance: Impact of (experimental) factors in an experiment on\nthe change / variation of the output to be examined.\n- Secondary variance: Variation of the output to be examined, caused by nuisance variables.\nNot in the focus of the study.\n- Error variance: Variation caused by measurement errors and random processes.\n\n> Note: Secondary and error variances are grouped to the residual variance.\n\n### Variantion of Variance\n\nThe variance of the dependent variable (DV) (primary variance) should be attributed to the systematic variation of the independent variable (IV). The secondary variance should be controlled and the error variance minimized.\n\n![Summary Variance](img/var_summary.png)\n\n#### Maximizing the primary variance\n\n- Relationship is linear: Selecting of extreme values in the IV. \n- Relationship were curvilinear: Selecting optimal increments of IV.\n- Relationship were unknown: Selecting as many increments of IV in the smallest steps as possible.\n\n#### Control of the secondary variance\n\n- Keeping constant: Keeping the experimental setup constant.\n- Repetition: Several measurements are repeated on the same trial objects.\n- Randomization: Trial objects are assigned randomly to Treatment and Control groups to eliminate systematic bias\n- Blocking: Trial objects are grouped into homogeneous blocks based on one or more influential\nvariables to reduce variability.\n- Covariate adjustment: Nuisance variables are included as covariates in the statistical model to account for their effects.\n\n#### Minimizing the error variance\n\n- Reliable measurement setup: Standardization of the experimental conditions\n- Sample size: Larger sample sizes reduce the impact of individual measurement errors\n- Suitable analytical methods: Use of robust estimators to account for heterogeneous error variance\n\n### Properties of measurement instruments\n\n- Objectivity: Objectivity of an instrument is given when the results are independent of personnel and calculation methods.\n- Reliability: Reliability is the degree to which an instrument produces the same result each time under comparable conditions.\n- Validity: Validity is the extent to which an instrument measures what was intended.\n\n# Lecture 04: Properties of DoE\n\n## Design of Experiments Types\n\n### Trial and error\n\nCombination of parameters have no structure and are mixed randomly. No idea what factors influence how.\n\n### One-factor-at-a-time\n\nVary the first factor and then measure fuel consumption. Keep the setting with the lowest consumption and then vary the next factor. Easy to implement, but interaction between factors are not recognized. Research question is answered neither systematically nor exhaustively.\n\n### Full factorial design\n\nTwo levels (+/-) are defined per factor. All possible combinations of factor levels are varied. All main effects and all interactions can be determined. Can be used as a screening experiment to identify potentially important variables. The effort involved increases rapidly as the number of factors increases. Each additional factor doubles the number of combinations.\n\n#### Profile Plot\n\nImpact of the factors on the dependent variable $x$. Based model:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\dots + \\beta_n x_n \n$$\n\n## Factorial design with interactions\n\nInteractions can occur in experiments with two or more independent variables. An interaction of two factors means that the two factors interact in a complex way. If there is an interaction, the effect of one factor depends on the levels of the other factor. Interaction terms are written as multiplication.\n\n- Two way interaction: $x_1 \\times x_2$\n- Three way interaction: $x_1 \\times x_2 \\times x_3$\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\times x_2\n$$\n\n### Full factorial designs\nGeneralization of two-level full factorial design with $k$ factors and $n$ levels. All possible factor combinations are varied.\n\n$$\n\\text{combinations} = n^k\n$$\n\n### Fractional factorial designs\n\nOnly a (balanced) part of the possible combinations of factors are varied.\n\n$$\n\\text{combinations} = n^{k-1}\n$$\n\n#### Design\n\n- Procedure: The factor levels are determined before the experiment\n- Factor combinations: Only a part of the possible combinations of factors are selected\n- Restrictions: In fractional factorial designs, interactions can only be partially measured because not all possible combinations of factor levels are tested.\n- Advantages: The effort involved is significantly lower compared to full factorial designs\n- Statistical analysis: As in the case of full factorial design, but without interactions\n\n## Quality criteria of experiments\n\n### Internal Validity\n\nExists when changes in dependent variables (DV) are attributed to independent variables (IV). Increases with decreasing impact of nuisance variables.\n\n#### Population Validity\n\nDegree to which the results of a study can be generalized from the sample to the whole population.\n\n#### Situation Validity\n\nDegree to which the findings of a study can be applied to different situations.\n\n### External Validity\n\nExists when experimental results from a sample can be generalized to the entire population. Increases with increasing naturalness.\n\n### Construct Validity\n\nEffectiveness of the measurement methods in precisely capturing the intended construct\n\n### Relationship between Internal vs. External Validity\n\n![Relationship between Internal vs. External Validity](img/internal_vs_external_validity.png)\n\nThe lowest general level of validity is at the bottom left for the quasi-experimental laboratory study, and the highest is at the top right for the experimental field study. A well-controlled lab experiment may maximize internal validity by eliminating confounding variables.\n\n# Lecture 05: Sampling\n\n## Population\n\nSet of all (potentially explorable) elements that have a common characteristic or a common combination of characteristics. The observation units (individuals, households, etc.) must be defined before the research start. In order to define the population, clear differentiation criteria must be formulated, so that for each observation unit it can be determined whether it is part of the population.\n\n- Differentiation in geographical aspects\n- Differentiation of temporal aspects (point in time or period)\n- Differentiation in factual / content-related aspects\n\n## Generalizing from a sample to the population\n\nThe population is often very large or not fully accessible. The population can be defined but not identified.\n\n### Characteristics of a sample\n\nA sample is a subset of all observation units and should reflect the relevant aspects of the population as accurately as possible. Three elements contribute to creating or describing representativeness:\n\n- The sample is drawn randomly.\n- Estimation procedure for generalizing from the sample to the population is reported.\n- Accuracy is reported, which is influenced by the sample size, among other things\n\n### Point estimate of the mean\n\nAn \"estimator\" is a function that calculates a value.\n\n- $\\bar{x}$: Mean of the sample\n- $\\mu_0$: True mean in the population (generally unknown)\n\n::: {.callout-important}\nThe mean value x of a sample is an unbiased, efficient and consistent estimator of the true mean value in the population: $\\mu_0 = E(\\bar{x})$\n:::\n\n## Sampling methods\n\n- Probabilistic (random) sampling procedures: Selecting elements based on a random mechanism\n- Non-probabilistic (non-random, purposive, arbitrary) sampling procedures: Selection of the elements is not based on a random mechanism, but is made by certain decisions (purposive or arbitrary selection of elements)\n\n### Sampling Map\n\n![Sampling procedure Overview](img/sampling_map.png)\n\n### Simple random sampling (SRS)\n\nRandom selection of n elements from the N elements of the population. Each element has the same probability of being included in the sample.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- rnorm(100) # 100 random numbers\nView(data)\n\ndata_sample <- sample(data, 10, replace=FALSE) # Take 10 random sample from data\nView(data_sample)\n```\n:::\n\n\n\n\n\n\n### Stratified random sampling\n\nStructure of the population regarding certain characteristics is known in advance. The population is stratified in accordance with these characteristics.\n\n- Proportional: Selection set in each Disproportional: Selection rate per stratum stratum is the same (self-weighting)\n- Disproportional: Selection rate per stratum stratum is the same (self-weighting) is different (samples will be weighted)\n\n### Cluster sampling\n \nThe elements are selected at a higher level. Variability within clusters is small (cluster elements are very similar) while variability between clusters is large (clusters differ greatly).\n\n### Arbitrary sampling\n\nArbitrary sampling occurs when a sample is selected based on the researcher's discretion, convenience or ease of availability, without applying a specific, structured or random procedure.\n\n### Targeted sampling\n\nFor populations that are difficult to reach and whose members are not closely networked. Preferred locations or places of residence of the members are identified and then systematically recruited on site.\n\n### Respondent Driven Sampling\n\nMultiple waves of peer-to-peer recruitment with statistical adjustments are conducted to approximate a random sample. Recruited individuals are only allowed to recruit a limited number of other individuals and are only rewarded for each person actually recruited.\n\n## Representativeness\n\nThe degree of representativeness is not measurable. The sample should be representative with regard to key characteristics of the study.\n\n## Sampling errors\n\n### Non-sampling error\n\nDifference in the mean value between the defined ideal population and the real population that cannot be attributed to deficiencies in the random selection of the sample.\n\n- Coverage error*: Part of the population cannot be identified.\n- Systematic non-response: Lack of information on certain individual elements.\n\n### Sampling Error\n\nDifference between the estimated mean value from a randomly drawn sample and the real mean value of the population.\n\n- Selection error: Not all elements of the population have the same selection probability.\n- Use of an unsuitable estimator.\n\n## Variability of Sample Means\n\nIn randomly drawn equal samples, the sample means vary depending on:\n\n- Attribute: The more heterogeneously an attribute is distributed in the population, the greater the variability of the sample means among many samples.\n- Sample size: The smaller the sample size, the greater the variability of sample means among many samples.\n\n## Standard Error\n\nThe standard error is a measure of the variability of the sample means among many samples. It quantifies the spread of sample means from repeated random samples of the same size\naround the population mean $\\mu_0$\n\n$$\n\\hat{\\sigma}_{\\bar{X}} = \\sqrt{\\dfrac{s^2}{n}} = \\dfrac{s}{\\sqrt{n}}\n$$\n\nThe larger the sample size, the smaller the standard error, and therefore, the larger the sample size, the more precise the sample mean is as an estimator of the population mean.\n\n# Lecture 06: Effect size & Power analysis\n\n## Statistical significance and importance of an effect\n\nThe chance of having a significant result in a hypothesis test is:\n\n- larger if sample size $n$ increases.\n- smaller if standard deviation $s$ increases.\n\nAn effect in the population can be specified by an effect measurement. This measurement results in what is referred to as effect size.\n\n> Note: The name effect size (ES) comes from Cohen (1992)\n\n## Effect size\n\nEffect size is a statistical measure that quantifies the magnitude (or strength) of a phenomenon. A tiny p-value in a very large study might show a difference that is statistically significant, but practically meaningless. Effect size helps to evaluate exactly this. It expresses this difference in terms of standard deviations.\n\n- A Cohen's d of 1.0 means the means of the two groups are separated by exactly one standard deviation.\n- A Cohen's d of 0.0 means there is no difference between the means.\n\n![Effect Size Table](img/effect_size_table.png)\n\n## Power analysis \n\nPower analysis is the crucial step in research design that connects your expected effect size with your desired power (80%) and significance (5%) to determine the minimum sample size you need to run a valid study.\n\n### Determining the sample size with R\n\n```r\ninstall.packages(\"pwr\")\nlibrary(pwr)\npwr.t.test(d = 0.5, power = 0.80, sig.level = 0.05)\n```\n\n# Lecture 07: Paradigms\n\n## The Fourth Paradigm\n\n1. Empirical Science: Description of natural phenomena.\n2. Theoretical Science: Modelling and generalization.\n3. Computational Science: Simulation of complex phenomena.\n4. Data-Intensive Science (eScience): Synthesis of information technology and science.\n\n## Quantitative empirical research\n\n1. Formulation of teh research of the study\n2. Planning and preparation of the study\n3. Data collection\n4. Data Analysis\n5. Reporting\n\n::: {.callout-note}\nDeductive approach → Conclusion from the general to the specific.\n:::\n\n### Limitations of quantitative empirical research\n\n- Meaning of significant hypotheses vs. meaning of effect size.\n- p-hacking (looking for data subsets and configurations until the p-value is less than 5%).\n- Assumptions about the distribution of variables are violated.\n- Assumption of homogeneity of variance is violated.\n\n## Data-driven research\n\n- Visual Analytics: Exploratory Data Analysis & Descriptive Statistics.\n- Machine Learning and Predictive Modelling: Regression / Classification / Decision Trees.\n- Advanced Analytics for Unstructured Data.\n\n::: {.callout-note}\nInductive approach → Conclusion from the specific to the general\n:::\n\n### Limitations of data-driven research\n\n- Big Data Hubris: Correlation is understood as causality.\n- Sparse data: Although the data basis is \"big,\" it contains little information.\n- Data analysis: A whole range of methodical errors.\n\n## Abductive approach and combination of approaches\n\nInduction: Search and generation of theories that fit the research context. Induction shows that something actually is operative.\nDeduction: Verification or falsification of existing theories. Deduction proves that something must be.\nAbduction: Search and generation of new, also speculative theories. Abduction merely suggests that something may be.\n\n> Note: Ideally, all three methods are used cyclically.\n\n## ANOVA\n\nThe Analysis of Variance (ANOVA) is a statistical test used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.\n\n- One-Way ANOVA: Used when comparing means across groups based on one single categorical independent variable (factor).\n- Two-Way ANOVA: Used when comparing means based on two or more independent variables (factors), allowing for the testing of interaction effects between the factors.\n\n## Key Steps in Analysis of Variance\n\n1. Design of experiment\n2. Calculating differences and sum of squares\n3. Verification of the model\n4. Considering other aspects (post hoc tests)\n5. Testing of assumptions (homogeneity of variance, etc.)\n6. Interpretation of the model and reporting (profile plots)\n\n> Note: Step 2 is done by the ANOVA model\n\n## One-way ANOVA in R\n\n```r\noneway.test(salary ~ factor(experience), var.equal=TRUE, data=data)\n```\n\n```r\nfit <- aov(salary ~ factor(experience), data=data)\nsummary(fit, intercept=TRUE)\n```\n\n## Two-Way ANOVA in R\n\n```r\n# Only main effects, no interaction\nfit <- avo(salary ~ factor(experience) + factor(position), data=data)\nsummary(fit, intercept=TRUE)\n\n# With Interaction\nfit <- avo(salary ~ factor(experience) * factor(position), data=data)\nsummary(fit, intercept=TRUE)\n```\n\n## Main Effects\n\nThe direct effect of an independent variable on the dependent variable is called main effect. Profile plots are used as visualization.\n\n![Profile Plots of Main Effect](img/profile_plot_main_effect.png)\n\n::: {.callout-note}\nIf the profile plot shows a (nearly) horizontal line, the main effect in question is probably not significant.\n:::\n\n## Interaction Effect\n\nAn interaction between experience and position means there is dependency between the two variables. The independent variables have a complex influence on the dependent variable. The factors do not just function additively but act together in a different manner.\n\n![Profile Plot of Interaction Effect](img/profile_plot_interaction_effect.png)\n\n\n::: {.callout-note}\nIf there is no interaction, the lines are parallel. If there is an interaction, the lines are not parallel.\n:::\n\n## Prerequisites of ANOVA\n\n- Sampling: Randomly assigning participants to the treatment or control groups.\n- Distribution of residuals: Residuals (= error) are normally distributed.\n- Homogeneity of variances: Residuals (= error) have constant variance\n- Balanced design: Same sample size in all groups.\n\n> Note: ANOVA is relatively robust against violations of prerequisites.\n\n## Post Hoc Tests\n\nThere are different methods to compare groups in pairs. All methods are similar, however, in that they solve the problem of multiple testing.\n\n```r\npairwise.t.test(x, y, p.adj=\"bonf\")\n```\n\n## Effect size\n\nPartial eta squared ${\\eta_p}^2$ relates the variance explained by one factor to the variance not explained by other factors in the model.\n\n```r\nlibrary(effectsize)\neta_squared(fit)$Eta2 # Use your ANOVA fit\n```\n\n## Overview over Statistical Hypothesis Tests\n\nChoosing the type of analysis depending on level of measurement\n\n![Overview over Statistical Hypothesis Tests](img/hyp_test_overview.png)\n\n# Lecture 08: A/B Testing\n\n## What is A/B testing?\n\nA/B testing (also bucket testing or split-run testing) is an experiment. The research questions are applied to two (A/B) or more randomized groups. The statistical analysis is done by t-test, ANOVA and more advanced methods.\n\n## Carrying out A/B testing\n\nThe aim is to determine which version achieves a better result, in terms of click-through rate etc. Two versions A and B are tested in parallel in a live environment. The generated data becomes the basis for decisions. Depending on the experiment, the test can be applyed to all users of a website or only a subset.\n\n## Bandit algorithm\n\nSeveral variants (treatments A, B, C, ...) are run in parallel. The variant with the highest \"success\" (according to metrics) get more data traffic. Other variants are refined and tested with proportionally less traffic. Traffic allocation is continuously adjusted based on the observed success. As the test progresses, more and more information about the performance of the variants becomes available, so that the most successful variant can be identified dynamically.\n\n::: {.callout-note}\nA key advantage of bandit algorithms is that they can achieve higher overall profit while still collecting data on the other variants.\n:::\n\n## Measurement / Metrics / KPI\n\n- Measurement: Method for obtaining one or more measured values that can be assigned to a quantity\n- Metrics: Calculation from measured values\n- KPI: Quantifiable metric that shows how effectively the most important company goals are achieved\n\n## Error sources and pitfalls\n\nA/B testing cannot be used in all research questions → Example: For a complete website redesign (further elements have to be included). Population is unknown, sampling procedure is not suitable, sampling bias, ect.\n\n### Dark Pattern / Deceptive Pattern\n\nDark / deceptive patterns are patterns of persuasion and influence. They may increase short-term gains for the provider but can harm brand image, user experience, and customer satisfaction. A/B testers must ensure that the variants they test are ethically acceptable and aim to create genuine added value for the user.\n\n::: {.callout-caution}\nThe problem with testing dark patterns is that A/B tests typically measure short-term metrics (such as clicks or conversions). They are not good at capturing long-term damage.\n:::\n\n# Lecture 09: Factorial designs\n\n## Full factorial designs\n\n### Single factorial (One)\n\nOne independent variable IV (factor) acts on the dependent variable DV. Two groups are compared:\n\n- 1 IV: one IV with two levels (dichotomous)\n- 1 DV: metric scaled\n- Examples: A/B test, RCT with treatment and control or two treatments\n- Analysis: One-way ANOVA\n\nOne independent variable IV (factor) acts on the dependent variable DV. Several groups are compared:\n\n- 1 IV: one IV with several levels\n- 1 DV: metric scaled\n- Examples: RCT with combinations of more than two treatments and control\n- Analysis: One-way ANOVA\n\n### Multifactorial (Several)\n\nSeveral factors act on the dependent variable DV. Several groups are compared:\n\n- X IV → several IV with two or more levels each\n- 1 DV → metric scaled\n- Examples: Dwell time with two IV\n- Analysis: Multi-factorial ANOVA (two-way ANOVA, three-way ANOVA, ...)\n\n## Fractional factorial designs\n\nThe more factors and the more characteristics, the larger the number of groups. Provided that there are no interactions, the experiment can also be conducted successfully\nwith a reduced number of groups. By using Latin squares or related designs, the number of groups required can be significantly reduced compared to full factorial designs.\n\n### In principle multifactorial (Several)\n\nSeveral factors act on the dependent variable DV. Several groups are compared:\n- X IV → several IV with two or more levels each\n- 1 DV → metric scaled\n- BUT Not all possible combinations are considered\n\n## Latin square\n\nA Latin squares design makes it possible to study the main effects of factors without having to observe all combinations of treatment levels.\n\n![Combination Table](img/latin_square_normal.png)\n\nAll combinations lead to a total of 27 (full factorial).\n\n![Latin Square](img/latin_square.png)\n\nHere we only use 9 combinations (fractional factorial).\n\n::: {.callout-caution}\nA Latin squares design can only be used if it follows from theory or empirical evidence that the joint effect of the factors does not produce interactions.\n:::\n\n# Lecture 10: Large data quantities\n\n## What are large data quantities?\n\n- Samples are taken as part of a study. Primary goal: To answer research questions\n- Administrative data are collected for various reasons. Primary goal: To serve documentary and administrative purposes\n- Grey area Data from full surveys (census), from social media and from \"representative\" surveys lie somewhat between data from a sample and administrative data.\n\nThis terminology has become established in many fields of research:\n- Made data → Data is generated by researchers (\"made\").\n- Found data → Data are obtained administratively and technically (\"found\")\n\n## What is bias?\n\nDeviation between mean $\\mu_0$ in the population and sample mean $\\bar{x}$. Three elements determine the bias:\n\n1. Data quality measure\n2. Data quantity measure\n3. Problem difficulty measure\n\n### How can bias be quantified?\n\n$$\n\\text{Bias} = \\bar{x} - \\mu_0 = \\rho_{R,G} \\times \\sqrt{\\frac{1-f}{f}} \\times \\sigma_G\n$$\n\n- $\\rho_{R,G}$: Data Quality\n- $\\sqrt{\\frac{1-f}{f}}$: Data Quantity\n- $\\sigma_G$: Problem Difficulty\n\n### Data defect correlation $\\rho_{R,G}$\n\nIn $\\rho_{R,G}$ the $R$ is a function that shows how data is obtained from the population. In simple random sampling, the $R$ function generates a randomly generated sequence of elements drawn from the population. Because of the random process, the selection of an element is independent of $G$.\n\n### Statistical paradises and paradoxes in relation to administrative data sets\n\nMeasure for the size of the bias → Mean-squared error (MSE). The MSE measures the deviation (bias) of the estimator $\\bar{x}$ from the mean $\\mu_0$ in the population. The bias goes to $0$ only, if the size $n$ of the administrative dataset goes against $N$ ($n$ → $N$).\n\n::: {.callout-important}\nThe absolute size $n$ of the administrative dataset is meaningless without specifying $N$.\n:::\n\n## Spurious Correlation\n\nA spurious correlation is a relationship between two variables that appears to be statistically significant and related, but is not actually caused by one another. The relationship is deceptive or coincidental, and often results from either a hidden factor or simply random chance. The primary risk of spurious correlation is that people might mistakenly conclude that a causal relationship exists based solely on the correlation, leading to incorrect policies or decisions.\n\n# Lecture 11: Experiments in social media\n\n## What is SoMe\n\nSocial media are interactive internet-based applications. It support user-generated content such as images, text, videos, and status updates and enable users to connect with one another through various interaction mechanisms, such as follows and likes (X), sharing and viewing reels (Instagram), engaging with trends (TikTok), or forming friendship connections (Facebook).\n\n## Communication in SoMe\n\n- One – e.g. X: Specific information is sent from one source to «everyone»\n- Two – e.g. SMS / WhatsApp: Exchange of information between two individuals via a certain medium.\n- Many – e.g. Facebook / WhatsApp: Information is exchanged in a group via social media actively or passively\n\n### Mode\n\nThe mode of interaction is also changed during a session. Users can speak, text, email, video chat, and post items on social media channels or blogs. The communication / interaction is not always carried out in the same mode. For example, a telephone call can be answered with a text message.\n\n## Population bias / Selection bias\n\nBias essentially means that the population being examined does not correspond to the defined population. If the defined population refers to the entire population and their subgroups, social media is by definition subject to bias. This is mainly due Sampling frame and Sampling procedure. Certain manifestations of these elements can be observed mainly (in some cases exclusively) in social media.\n\n## Sources of bias in social media\n\n- Activity bias I: Active (only) at the time of the study / data collection / storage.\n- Activity bias II: A few users are very active on social media, while most users use social media only passively.\n- Activity with bots: Programs that behave like users or react to specific triggers.\n\n## Standardization of bias in social media\n\n- Medium & platform: Restrictions on access to data / (unknown) ways and methods on how data is stored\n- Survey / sampling & representativeness: Not taking into account the circumstances, e.g. in the case of a storm on X\n- Ways of managing data and sources: Reproducibility limited by restrictive access to data / by deletion of items\n\n## Research with social media\n\nStudies using data from social media have great potential for investigating research questions in social science / psychological research questions that are new or place special demands on the study design.\n\n- Rapid availability of data / information and continuous updating\n- Simple and low-cost extraction processes compared to classical surveys\n\n> Note: Data from social media is most likely «found data».\n\n## Methods of data collection in SoMe\n\n- Using the functionalities of social media: Use individual snowball sampling\n- Paid access to survey: Place ads in social media, Use survey tools or Include specialized platforms (Amazon Mechanical Turk)",
    "supporting": [
      "doe_notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}